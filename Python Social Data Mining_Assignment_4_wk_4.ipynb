{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Week 4 Assignment Questions\n",
      "\n",
      "\n",
      "#Question 1.\n",
      "\n",
      "**Overview:** This question provides you with practice implementing graphs in native Python.\n",
      "\n",
      "**Point total:** ungraded, immediate feedback\n",
      "\n",
      "**Time estimate:** 20 minutes.\n",
      "\n",
      "Examine the example social network below (from [FMS Advanced Systems Group](http://www.fmsasg.com/SocialNetworkAnalysis/)). Create a variable named `graph` that represents this network using an **adjacency matrix** in native Python (not networkx). Calculate and display the degree of every node in the matrix.\n",
      "\n",
      "![Example network](http://www.fmsasg.com/SocialNetworkAnalysis/SocialNetworkAnalysis_Degrees.gif)\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Question 2.\n",
      "\n",
      "**Overview:** This question provides you with practice representing and drawing graphs using networkx.\n",
      "\n",
      "**Point total:** 5 points.\n",
      "\n",
      "**Time estimate:** 30 minutes.\n",
      "\n",
      "Represent the graph from Question 1 using networkx. Draw the graph."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Question 3.\n",
      "\n",
      "**Overview:** For this question, you will infer a graph of relationships between movies. Two movies will be connected if people rate them similarly. You will draw your graph using networkx.\n",
      "\n",
      "**Point total:** 15 points.\n",
      "\n",
      "**Time estimate:** 1 hour.\n",
      "\n",
      "\n",
      "For this question, we'll infer a graph of related movies. Our graph will visualize relationships between movies in the [MovieLens 1 Million dataset](http://grouplens.org/datasets/movielens/) (a smaller dataset than the earlier one we studied). Run the following snippet of code to download and unzip the MovieLens 1M dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import urllib\n",
      "import zipfile\n",
      "\n",
      "# downloads the zip file\n",
      "if not os.path.isfile('ml-1m.zip'):\n",
      "    print('downloading ml-1m.zip...')\n",
      "    data = urllib.urlopen('http://files.grouplens.org/datasets/movielens/ml-1m.zip').read()\n",
      "    print('downloaded %d bytes' % len(data))\n",
      "    f = open('ml-1m.zip', 'w')\n",
      "    f.write(data)\n",
      "    f.close()\n",
      "    \n",
      "# extracts the ratings.dat\n",
      "if not os.path.isfile('ml-1m/ratings.dat'):\n",
      "    print('extracting zip file.')\n",
      "    zip = zipfile.ZipFile('ml-1m.zip')\n",
      "    zip.extractall()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "downloading ml-1m.zip...\n",
        "downloaded 6008687 bytes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "extracting zip file.\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I've provided you with a series of helper functions below to interact with the movie data. The two most important functions for you are `get_cosimilarities()` and `read_titles()`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "def read_titles():\n",
      "    \"\"\"\n",
      "        Returns a dictionary mapping movie ids to movie titles.\n",
      "    \"\"\"\n",
      "    titles = {}\n",
      "    for line in open('ml-1m/movies.dat'):\n",
      "        (m, title, genres) = line.split('::')\n",
      "        titles[int(m)] = title.decode('latin-1').encode('ascii', 'ignore')\n",
      "    return titles\n",
      "\n",
      "def get_cosimilarities():\n",
      "    \"\"\"\n",
      "        Returns a cosimilarity matrix between movies with 1000 or more ratings.\n",
      "        Cosimilarity values are average-adjusted cosine similarities between -1.0 and 1.0.\n",
      "        The matrix has structure:\n",
      "        \n",
      "        {\n",
      "            movieid1 : {\n",
      "                movieid2 : 1.0,\n",
      "                movieid2 : 0.5,\n",
      "                movieid3 : 0.4,\n",
      "                ....\n",
      "            },\n",
      "            movieid2 : {\n",
      "                movieid1 : 0.5,\n",
      "                movieid2 : 1.0,\n",
      "                movieid3 : 0.9,\n",
      "            },\n",
      "            ...\n",
      "        }\n",
      "    \"\"\"\n",
      "    print('reading movie means...')\n",
      "    means = get_movie_means(1000)\n",
      "    print('read means for %d movies' % len(means))\n",
      "    \n",
      "    xdotx = defaultdict(lambda: defaultdict(float))\n",
      "    xdoty = defaultdict(lambda: defaultdict(float))\n",
      "    ydoty = defaultdict(lambda: defaultdict(float))\n",
      "    \n",
      "    users = read_users()\n",
      "    for (i, u) in enumerate(users):\n",
      "        if i % 500 == 0:\n",
      "            print('processing user %d of %d' % (i, len(users)))\n",
      "        good_movies = set(u.keys()).intersection(means.keys())\n",
      "        for m1 in good_movies:\n",
      "            x = u[m1] - means[m1]\n",
      "            for m2 in good_movies:                \n",
      "                y = u[m2] - means[m2]\n",
      "                xdotx[m1][m2] += x * x\n",
      "                ydoty[m1][m2] += y * y\n",
      "                xdoty[m1][m2] += x * y\n",
      "    \n",
      "    print('building cosimilarity matrix')\n",
      "    cosims = defaultdict(dict)\n",
      "    for m1 in xdotx:\n",
      "        for m2 in xdotx[m1]:\n",
      "            sim = xdoty[m1][m2] / (xdotx[m1][m2] * ydoty[m1][m2]) ** 0.5\n",
      "            cosims[m1][m2] = sim\n",
      "    \n",
      "    return cosims\n",
      "\n",
      "def read_ratings():\n",
      "    \"\"\"\n",
      "        A generator over movie ratings. \n",
      "        Each generated record represents a single movie rating with fields user, movie, rating, and timestamp.\n",
      "    \"\"\"\n",
      "    f = open('ml-1m/ratings.dat')\n",
      "    for line in f:\n",
      "        (user, movie, rating, tstamp) = line.strip().split('::')\n",
      "        yield { 'user' : int(user), 'movie' : int(movie), 'rating' : int(rating), 'tstamp' : tstamp }\n",
      "    f.close()\n",
      "\n",
      "def read_users():\n",
      "    \"\"\"\n",
      "        Returns a list of users.\n",
      "        Each user is a dictionary whose keys are movie ids (ints) and values are ratings (also ints).\n",
      "    \"\"\"\n",
      "    users = defaultdict(dict)\n",
      "    for rating in read_ratings():\n",
      "        u = rating['user']\n",
      "        m = rating['movie']\n",
      "        r = rating['rating']\n",
      "        users[u][m] = r\n",
      "    return users.values()\n",
      "\n",
      "def get_movie_means(min_ratings_per_movie):\n",
      "    \"\"\"\n",
      "        Returns a dictionary mapping movie id to movie mean.\n",
      "        Limits the returned movies to those with at least min_ratings_per_movie ratings.\n",
      "    \"\"\"\n",
      "    sums = defaultdict(float)\n",
      "    counts = defaultdict(int)\n",
      "    for rating in read_ratings():\n",
      "        m = rating['movie']\n",
      "        r = rating['rating']\n",
      "        sums[m] += r\n",
      "        counts[m] += 1\n",
      "    means = {}\n",
      "    for m in counts:\n",
      "        if counts[m] >= min_ratings_per_movie:\n",
      "            means[m] = sums[m] / counts[m]\n",
      "    return means\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You'll want to just build the cosimilarity matrix once, because it takes 30 seconds or so..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosims = get_cosimilarities()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "reading movie means...\n",
        "read means for 207 movies"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 0 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 1000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 1500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 2000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 2500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 3000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 3500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 4000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 4500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 5000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 5500 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "processing user 6000 of 6040"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "building cosimilarity matrix"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the cosimilarity matrix is built, you can experiment with it. For example, the following bit of code displays the 10 movies with highest and lowest correlations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_cosims(cosims):\n",
      "    titles = read_titles()\n",
      "    all_cors = []\n",
      "    for m1 in cosims:\n",
      "        for m2 in cosims[m1]:\n",
      "            if m1 != m2:\n",
      "                all_cors.append((cosims[m1][m2], m1, m2))\n",
      "\n",
      "    # tuples are sorted by first element, which is cosimilarity\n",
      "    all_cors.sort()\n",
      "    \n",
      "    # skip every other pair of movies because each pair shows up twice and sim(m1, m2) == sim(m2, m1)\n",
      "    print('Lowest correlations:')\n",
      "    for (sim, m1, m2) in all_cors[:20:2]:\n",
      "        print('  %.3f: %s (%d)   <->   %s (%d)' %  (sim, titles[m1], m1, titles[m2], m2))\n",
      "\n",
      "    all_cors.reverse()\n",
      "    print('\\nHighest correlations:')\n",
      "    for (sim, m1, m2) in all_cors[:20:2]:\n",
      "        print('  %.3f: %s (%d)   <->   %s (%d)' %  (sim, titles[m1], m1, titles[m2], m2))\n",
      "\n",
      "test_cosims(cosims)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lowest correlations:\n",
        "  -0.216: Annie Hall (1977) (1230)   <->   Patriot, The (2000) (3753)\n",
        "  -0.186: Air Force One (1997) (1608)   <->   Boogie Nights (1997) (1673)\n",
        "  -0.185: Armageddon (1998) (1917)   <->   Rushmore (1998) (2395)\n",
        "  -0.162: Ghost (1990) (587)   <->   This Is Spinal Tap (1984) (1288)\n",
        "  -0.151: This Is Spinal Tap (1984) (1288)   <->   Armageddon (1998) (1917)\n",
        "  -0.146: Edward Scissorhands (1990) (2291)   <->   Patriot, The (2000) (3753)\n",
        "  -0.142: Annie Hall (1977) (1230)   <->   Green Mile, The (1999) (3147)\n",
        "  -0.139: Citizen Kane (1941) (923)   <->   Lost World: Jurassic Park, The (1997) (1544)\n",
        "  -0.137: True Lies (1994) (380)   <->   This Is Spinal Tap (1984) (1288)\n",
        "  -0.135: Independence Day (ID4) (1996) (780)   <->   Annie Hall (1977) (1230)\n",
        "\n",
        "Highest correlations:\n",
        "  0.712: Godfather: Part II, The (1974) (1221)   <->   Godfather, The (1972) (858)\n",
        "  0.703: Lethal Weapon 2 (1989) (2001)   <->   Lethal Weapon (1987) (2000)\n",
        "  0.690: Austin Powers: The Spy Who Shagged Me (1999) (2683)   <->   Austin Powers: International Man of Mystery (1997) (1517)\n",
        "  0.687: Patriot Games (1992) (3256)   <->   Clear and Present Danger (1994) (349)\n",
        "  0.664: Star Wars: Episode V - The Empire Strikes Back (1980) (1196)   <->   Star Wars: Episode IV - A New Hope (1977) (260)\n",
        "  0.638: Back to the Future Part III (1990) (2012)   <->   Back to the Future Part II (1989) (2011)\n",
        "  0.634: Star Wars: Episode VI - Return of the Jedi (1983) (1210)   <->   Star Wars: Episode V - The Empire Strikes Back (1980) (1196)\n",
        "  0.622: Toy Story 2 (1999) (3114)   <->   Toy Story (1995) (1)\n",
        "  0.584: Back to the Future Part II (1989) (2011)   <->   Back to the Future (1985) (1270)\n",
        "  0.579: Star Wars: Episode VI - Return of the Jedi (1983) (1210)   <->   Star Wars: Episode IV - A New Hope (1977) (260)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that you have built the correlation matrix and understand how to use it, it's time to get to work!\n",
      "\n",
      "***Your task:*** First, create a networkx graph capturing relationships between movies. Each movie should have an edge to its closest `n` neighbors. You can experiment with different values for `n` (I liked `n = 3`). Draw your graph using networkx.\n",
      "\n",
      "***Hint:*** The algorithm will closely follow the last cell of code in Section 7 of the lecture notes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Question 4.\n",
      "\n",
      "**Overview:** In this question you'll independent use Gephi to visualize and explore the movie graph.\n",
      "\n",
      "**Time estimate:** 45 minutes.\n",
      "\n",
      "**Point total:** 10.\n",
      "\n",
      "Follow the procedure I demoed:\n",
      "\n",
      "1. Import your networkx graph into Gephi following the procedure you learned in the lecture notes.\n",
      "2. Color code the communities in the graph using the modularity partitioning.\n",
      "3. Fine tune the graph and its layout so it is easy to read.\n",
      "4. Upload a picture of your final graph (from the preview tab) to the course website."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}