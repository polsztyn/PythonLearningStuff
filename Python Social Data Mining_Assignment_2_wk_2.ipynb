{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Practice Exercise 1.\n",
    "\n",
    "**Time estimate:** 20 minutes\n",
    "\n",
    "Study the `make_tf_vector()` function in Section 1 (reproduced below). Answer the three questions below. You do not need to write code... just explain in words.\n",
    "\n",
    "* What does `doc.split()` precisely do? <font color=green> <b> doc.split breaks apart a string and returns a list of the words.  by default split divides on all whitespace characters.</b></font>\n",
    "* What are its limitations introduced to `make_tf_vector` when `split()`  is used like this? <font color=green> <b> One limitation is periods and other punction marks will be parsed together into a phrase.  Another would be that the same word in different cases would not be seen as equivent. </b></font>\n",
    "* How might you improve it? <font color=green> <b> Regular expressions could be used to convert the raw input text and remove all non alpha characters and change the case. </b></font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1})]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def make_tf_vector(doc):\n",
    "    \"\"\"\n",
    "        Calculates the term frequency of a document.\n",
    "        Given a string, splits it into words on whitespace.\n",
    "        Returns a dictionary mapping words to their frequency.\n",
    "    \"\"\"\n",
    "    v = defaultdict(int)\n",
    "    for term in doc.split():\n",
    "        v[term] += 1\n",
    "    return v\n",
    "\n",
    "tf_matrix = []\n",
    "for doc in docs:    \n",
    "    v = make_tf_vector(doc)    \n",
    "    tf_matrix.append(v)\n",
    "\n",
    "pprint(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Practice Exercise 2.\n",
    "\n",
    "**Time estimate:** 20 minutes.\n",
    "\n",
    "The Python code for the `l2norm()` function in Section 2 (shown below) uses a [list comprehension](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html). Rewrite the l2norm function using a standard \"for-each\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}), 'is', 3.4641016151377544)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}), 'is', 3.162277660168379)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1}), 'is', 2.449489742783178)\n"
     ]
    }
   ],
   "source": [
    "def l2norm(tf_vector):\n",
    "    \"\"\"\n",
    "        Returns the l2-norm (i.e. Euclidean length) of a vector.\n",
    "    \"\"\"\n",
    "    return sum([x*x for x in tf_vector.values()]) ** 0.5\n",
    "\n",
    "for tf_vector in tf_matrix:\n",
    "    norm = l2norm(tf_vector)\n",
    "    print('l2norm of' , tf_vector, 'is', norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}), 'is', 3.4641016151377544)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}), 'is', 3.162277660168379)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1}), 'is', 2.449489742783178)\n"
     ]
    }
   ],
   "source": [
    "def l2norm_simple(tf_vector):\n",
    "    \"\"\"\n",
    "        Returns the l2-norm (i.e. Euclidean length) of a vector. without list comprehension\n",
    "    \"\"\"\n",
    "    sum_sq = 0\n",
    "    for x in tf_vector.values():\n",
    "        sum_sq = sum_sq + (x*x)\n",
    "    \n",
    "    return (sum_sq)**0.5\n",
    "\n",
    "for tf_vector in tf_matrix:\n",
    "    norm2 = l2norm_simple(tf_vector)\n",
    "    print('l2norm of' , tf_vector, 'is', norm2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 1: \n",
    "\n",
    "**Time estimate:** 20 minutes\n",
    "\n",
    "The Python code for the normalize function (below) uses a list comprehension. Rewrite the normalize function using a standard \"for-each\" loop. Check your answer against the results posted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
      "3.46410161514\n",
      "{'me': 0.5773502691896258, 'Julie': 0.2886751345948129, 'loves': 0.5773502691896258, 'Linda': 0.2886751345948129, 'than': 0.2886751345948129, 'more': 0.2886751345948129}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "        Given a dictionary representing a sparse vector, returns a new rescaled vector.\n",
    "        The new vector contains values from the original vector rescaled by a constant.\n",
    "        The new vector will have an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    norm = l2norm(vector)\n",
    "    if norm == 0.0:\n",
    "        return dict(vector)\n",
    "    \n",
    "    return dict([(term, tf/norm) for (term, tf) in vector.items()])\n",
    "\n",
    "v = {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
    "print(v)\n",
    "print(l2norm(v))\n",
    "print(normalize(v))\n",
    "print(l2norm(normalize(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
      "3.46410161514\n",
      "{'me': 0.5773502691896258, 'loves': 0.5773502691896258, 'Linda': 0.2886751345948129, 'Julie': 0.2886751345948129, 'than': 0.2886751345948129, 'more': 0.2886751345948129}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize_simple(vector):\n",
    "    \"\"\"\n",
    "        Given a dictionary representing a sparse vector, returns a new rescaled vector.\n",
    "        The new vector contains values from the original vector rescaled by a constant.\n",
    "        The new vector will have an l2-norm of 1.0.  \n",
    "    \"\"\"\n",
    "    norm = l2norm(vector)\n",
    "    if norm == 0.0:\n",
    "        return dict(vector)\n",
    "    \n",
    "    norm_vector = {}\n",
    "    for (term,tf) in vector.items():\n",
    "         norm_vector[term] = tf/norm\n",
    "    return dict(norm_vector)\n",
    "\n",
    "\n",
    "v = {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
    "print(v)\n",
    "\n",
    "print(l2norm(v))\n",
    "print(normalize_simple(v))\n",
    "print(l2norm(normalize_simple(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 2:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Answer two questions about the `make_doc_frequency` function (below):\n",
    "\n",
    "1. Notice the use of `set()` in the inner-most for loop. Why is this necessary? How (specifically) would the result change if we didn't use it? <font color = green><b> The set casting fuction converts the list to a set.  This removes duplicates, so each term in a document will only be addressed by the inner loop once.  If we didn't convert the set to a list we would report a much higher and false document frequency value </font></b>\n",
    "2. The performance of `make_doc_frequency` is linear, but linear *in what?*  Does the performance of this method scale with the number of documents, the number of unique terms, or total word count across all documents? Explain your answer. Hint: All set operations require constant time, as do dictionary sets and gets. <font color=green><b>I'm really not sure of my answer here, because my first guess because of the nested loops would be that the perforance of this algorithm would be quadratic based on the number of documents and terms in each document.  But based on your hint I would guess that the performance of this algorihm would scale linearly as a fuction of the total word count accross all documents.</font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'me': 2, 'basketball': 1, 'Julie': 2, 'baseball': 1, 'likes': 2, 'loves': 2, 'Jane': 1, 'Linda': 1, 'He': 1, 'than': 3, 'more': 3})\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_doc_frequency(docs):\n",
    "    \"\"\"\n",
    "        Given a collection of documents (Strings), \n",
    "        returns a dictionary mapping each word to the number of times it appears.        \n",
    "        Words are split on whitespace.\n",
    "    \"\"\"\n",
    "    df = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for term in set(d.split()):\n",
    "            df[term] += 1\n",
    "    return df\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "print(make_doc_frequency(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 3:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "$$ TfIdf(doc, word) = tf(doc, word) * log\\frac{td}{1 + df(word)}$$\n",
    "Complete the `make_tf_idf_vector` function below. The steps you need to complete are outlined in the comments for the method. Once you correctly complete the function, and run the test that appears below it, you should see the following output:\n",
    "\n",
    "        Julie loves me more than Linda loves me\n",
    "                  Linda: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=2 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        Jane likes me more than Julie loves me\n",
    "                   Jane: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        He likes basketball more than baseball\n",
    "             basketball: tf=1 tf-idf=+0.500\n",
    "               baseball: tf=1 tf-idf=+0.500\n",
    "                     He: tf=1 tf-idf=+0.500\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.354\n",
    "                   more: tf=1 tf-idf=-0.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julie loves me more than Linda loves me\n",
      "          Linda: tf=1 tf-idf=+0.706\n",
      "             me: tf=2 tf-idf=+0.000\n",
      "          Julie: tf=1 tf-idf=+0.000\n",
      "          loves: tf=2 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.501\n",
      "           more: tf=1 tf-idf=-0.501\n",
      "\n",
      "Jane likes me more than Julie loves me\n",
      "           Jane: tf=1 tf-idf=+0.706\n",
      "             me: tf=2 tf-idf=+0.000\n",
      "          Julie: tf=1 tf-idf=+0.000\n",
      "          likes: tf=1 tf-idf=+0.000\n",
      "          loves: tf=1 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.501\n",
      "           more: tf=1 tf-idf=-0.501\n",
      "\n",
      "He likes basketball more than baseball\n",
      "     basketball: tf=1 tf-idf=+0.500\n",
      "       baseball: tf=1 tf-idf=+0.500\n",
      "             He: tf=1 tf-idf=+0.500\n",
      "          likes: tf=1 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.354\n",
      "           more: tf=1 tf-idf=-0.354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "def make_tf_idf_vector(td, df, doc):\n",
    "    \"\"\"\n",
    "        Given a total document count (td), document frequency dictionary (words -> # docs), and a document (a string)\n",
    "        Returns a tf_idf vector.\n",
    "        The returned vector is normalized so that it has an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    # step 1: Calculate the tf vector\n",
    "    tf=make_tf_vector(doc)\n",
    "    \n",
    "    \n",
    "    # step 2: Translate the tf vector into a tf-idf vector using the formula above\n",
    "    tf_idf_vector = {}\n",
    "    for term in tf:\n",
    "        tf_idf = float(tf[term])*log(float(td)/(1.0+float(df[term]))) \n",
    "        tf_idf_vector[term]=tf_idf\n",
    "\n",
    "    # step 3: Normalize the tf-idf vector so it has an l2 norm of 1.0\n",
    "    tf_idf_vector_norm = normalize(tf_idf_vector)       \n",
    "\n",
    "    # step 4: return the normalized tf-idf vector.\n",
    "    return tf_idf_vector_norm\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "td = len(docs)\n",
    "df = make_doc_frequency(docs)\n",
    "\n",
    "for d in docs:\n",
    "    tf = make_tf_vector(d)\n",
    "    tf_idf = make_tf_idf_vector(td, df, d)\n",
    "    print(d)\n",
    "    for term in sorted(tf_idf, key=tf_idf.get, reverse=True):\n",
    "        print('%15s: tf=%d tf-idf=%+.3f' % (term, tf[term], tf_idf[term]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 4:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Write a function called `sk_vector_to_simple_row` that returns a traditional (native) sparse vector representation for a row in a sci-kit learn matrix. Recall that the traditional python representation is a dict whose keys are terms and values are frequencies. Pattern your function after the example code in Section 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'julie': 1, 'loves': 2, 'linda': 1, 'than': 1, 'more': 1}\n",
      "{'me': 2, 'julie': 1, 'likes': 1, 'loves': 1, 'jane': 1, 'than': 1, 'more': 1}\n",
      "{'basketball': 1, 'baseball': 1, 'likes': 1, 'he': 1, 'than': 1, 'more': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def sk_vector_to_simple_row(lexicon, row):\n",
    "    \"\"\"\n",
    "    Given a sci-kit learn row vector, return a native Python sparse vector.\n",
    "    The result will be a dictionary whose keys are terms and values are term frequencies.\n",
    "    Pattern this function after the code in Section 4.2.\n",
    "    \"\"\"\n",
    "    python_simple_tf_vector = {}\n",
    "    for i in range(len(row.data)):\n",
    "        index = row.indices[i]        \n",
    "        term = str(lexicon[index])\n",
    "        val = row.data[i]\n",
    "        python_simple_tf_vector[term] = val\n",
    "        #print('\\t%s (id=%d) : %.3f' % (term, index, val))\n",
    "    return (python_simple_tf_vector)\n",
    "\n",
    "    \n",
    "    \n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(docs)\n",
    "lexicon =  count_vectorizer.get_feature_names()\n",
    "sk_tf_matrix = count_vectorizer.transform(docs)\n",
    "\n",
    "for row in sk_tf_matrix:\n",
    "    print(sk_vector_to_simple_row(lexicon, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 5:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Study the code at the end of section 5 (measuring vector similarity). Notice that \"by hand\" we calculate the result of the `sk_dot` function between each pair of documents.\n",
    "\n",
    "Write a function that calculates the similarity between each pair of documents using for loops. The output of this function should look approximately like:\n",
    "\n",
    "    Similarity between document 0 and 1 is 0.753602532225\n",
    "    Similarity between document 0 and 2 is 0.128408027002\n",
    "    Similarity between document 1 and 2 is 0.128408027002\n",
    "    \n",
    "Use the following bit of python code to setup the sci-kit tf-idf matrix you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between document 0 and 1 is 0.753602532225\n",
      "Similarity between document 0 and 2 is 0.128408027002\n",
      "Similarity between document 1 and 2 is 0.257423195662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Train a tf-idf transformer on the dataset\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(sk_tf_matrix)\n",
    "sk_tf_idf_matrix = tfidf.transform(sk_tf_matrix)\n",
    "\n",
    "\n",
    "def sk_dot(v1, v2):\n",
    "    return v1.dot(v2.transpose())[0,0]\n",
    "\n",
    "matrixShape = sk_tf_idf_matrix.shape\n",
    "numDocs = matrixShape[0]\n",
    "\n",
    "d0 = sk_tf_idf_matrix[0]\n",
    "d1 = sk_tf_idf_matrix[1]\n",
    "d2 = sk_tf_idf_matrix[2]\n",
    "\n",
    "import itertools\n",
    "a= itertools.combinations(range(numDocs),2)\n",
    "for x in a:\n",
    "    print 'Similarity between document ' + str(x[0]) + ' and ' + str(x[1])+ ' is ' + str( sk_dot(sk_tf_idf_matrix[x[0]],sk_tf_idf_matrix[x[1]] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:** The `shape` attribute of a matrix returns a two-tuple of the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n"
     ]
    }
   ],
   "source": [
    "print(sk_tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 6.\n",
    "\n",
    "**Time estimate:** One hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For questions 6, 7, and 8 you will use a dataset I collected from Wikipedia that contains the article text for all the [Academy Award Winning Films on Wikipedia](http://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films). After next week, you'll know how to collect this data yourself!\n",
    "\n",
    "The dataset is available on the course website called award_winners.zip. If you extract this file, you'll get a file called \"award_winners.txt.\" The format of the dataset is tab delimited, where the \"text\" field captures the extracted text of the Wikipedia article associated with the movie.\n",
    "\n",
    "    movie_title1       url1           text1\n",
    "    movie_title2       url2           text2\n",
    "    movie_title3       url3           text3    \n",
    "    .......\n",
    "    \n",
    "First, complete the generator function called readMovies below (we learned about generators last week). Each record generated by the file should be a dictionary with three keys: 'title', 'url', 'id', and 'text'. The ids should be assigned consecutive, and start with 0.\n",
    "\n",
    "You can use the `testReadMovies` function below to test your readMovies code. You should **not need to alter it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'C:/PythonLearningStuff/movie_sample.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-c2c35fd051e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m#testReadMovies('movies.txt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/PythonLearningStuff/movie_sample.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmovie\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_users\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mmovie\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-c2c35fd051e5>\u001b[0m in \u001b[0;36mread_users\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmovieID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# tokens is a list of the four values in each record\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'C:/PythonLearningStuff/movie_sample.txt'"
     ]
    }
   ],
   "source": [
    "def read_users(path):\n",
    "    movieID = 0\n",
    "    \n",
    "    for line in open(path):\n",
    "        tokens = line.split('\\t')    # tokens is a list of the four values in each record\n",
    "        title = int(tokens[0])\n",
    "        url = int(tokens[1])\n",
    "        text = float(tokens[2])\n",
    "        movieRec = {\"title\":title, \"url\":url, \"id\":movieID, \"text\":text}  \n",
    "        movieID = movieID+1    \n",
    "        yield movieRec\n",
    "\n",
    "\n",
    "def readMovies(path):\n",
    "    \"\"\"\n",
    "    Returns a generator that yields a record for each movie in the specified movie.txt.\n",
    "    The format of a single record is {\n",
    "        'id' : 0,\n",
    "        'title' : '12 Years a Slave',\n",
    "        'url' : 'http://en.wikipedia.org/wiki/12_Years_a_Slave_(film)',\n",
    "        'text' : '12 Years a Slave is a 2013 British-Amer...'        \n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "def testReadMovies(path):\n",
    "    \"\"\"Prints debugging information about the movies.txt\"\"\"\n",
    "    for movie in readMovies(path):\n",
    "        if movie['id'] <= 3 or movie['id'] >= 1193:                \n",
    "            print(\"============================ Movie %d ============================\" % movie['id'])\n",
    "            print(\"'%s'\" % movie['title'])\n",
    "            print(movie['url'])\n",
    "            print(movie['text'][:80] + '...')\n",
    "\n",
    "#testReadMovies('movies.txt')\n",
    "path = 'C:/PythonLearningStuff/movie_sample.txt'\n",
    "for movie in read_users(path):\n",
    "    print movie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you successfully complete The output from running your should be **exactly**:\n",
    "\n",
    "    ============================ Movie 0 ============================\n",
    "    '12 Years a Slave'\n",
    "    http://en.wikipedia.org/wiki/12_Years_a_Slave_(film)\n",
    "    12 Years a Slave is a 2013 British-American historical drama film and an adaptat...\n",
    "    ============================ Movie 1 ============================\n",
    "    '20 Feet from Stardom'\n",
    "    http://en.wikipedia.org/wiki/20_Feet_from_Stardom\n",
    "    20 Feet from Stardom is an Oscar-winning 2013 American documentary film directed...\n",
    "    ============================ Movie 2 ============================\n",
    "    '20,000 Leagues Under the Sea'\n",
    "    http://en.wikipedia.org/wiki/20,000_Leagues_Under_the_Sea_(1954_film)\n",
    "    20,000 Leagues Under the Sea is a 1954 American adventure film starring Kirk Dou...\n",
    "    ============================ Movie 3 ============================\n",
    "    '2001: A Space Odyssey'\n",
    "    http://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film)\n",
    "    2001: A Space Odyssey is a 1968 British-American science fiction film produced a...\n",
    "    ============================ Movie 1193 ============================\n",
    "    'Zorba the Greek (Alexis Zorbas)'\n",
    "    http://en.wikipedia.org/wiki/Zorba_the_Greek_(film)\n",
    "    Zorba the Greek (Greek title: Αλέξης Ζορμπάς, Alexis Zorba(s))is a ...\n",
    "    ============================ Movie 1194 ============================\n",
    "    'tom thumb'\n",
    "    http://en.wikipedia.org/wiki/Tom_thumb_(film)\n",
    "    Tom Thumb (stylised as tom thumb) is a 1958 fantasy-musical film directed by Geo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 7.\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Next, complete the `readMovieDocs()` function below, which  returns a list of all the movie documents, in order. Also complete the `readMovieTitles()` that does the same thing for titles. You can use the `testReadDocsAndTitles` function below to make sure they are working properly.\n",
    "\n",
    "***Hint:*** You should make good use of your `readMovies` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMovieDocs(path):\n",
    "    \"\"\"Returns a list of strings representing the movie articles text in the specified movies.txt\"\"\"\n",
    "\n",
    "def readMovieTitles(path):\n",
    "    \"\"\"Returns a list of strings representing the movie titles in the specified movies.txt\"\"\"\n",
    "    \n",
    "def testReadDocsAndTitles(path):\n",
    "    docs = readMovieDocs(path)\n",
    "    for i, d in enumerate(docs):\n",
    "        if i <= 3 or i >= 1193:          \n",
    "            print('doc %d is: %s' % (i, d[:50]))\n",
    "    print('\\n')\n",
    "            \n",
    "    titles = readMovieTitles(path)\n",
    "    for i, t in enumerate(titles):\n",
    "        if i <= 3 or i >= 1193:          \n",
    "            print('title %d is: %s' % (i, t))\n",
    "\n",
    "#testReadDocsAndTitles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should output exactly:\n",
    "\n",
    "        doc 0 is: 12 Years a Slave is a 2013 British-American histor\n",
    "        doc 1 is: 20 Feet from Stardom is an Oscar-winning 2013 Amer\n",
    "        doc 2 is: 20,000 Leagues Under the Sea is a 1954 American ad\n",
    "        doc 3 is: 2001: A Space Odyssey is a 1968 British-American s\n",
    "        doc 1193 is: Zorba the Greek (Greek title: Αλέξης Ζορ�\n",
    "        doc 1194 is: Tom Thumb (stylised as tom thumb) is a 1958 fantas\n",
    "        \n",
    "        \n",
    "        title 0 is: 12 Years a Slave\n",
    "        title 1 is: 20 Feet from Stardom\n",
    "        title 2 is: 20,000 Leagues Under the Sea\n",
    "        title 3 is: 2001: A Space Odyssey\n",
    "        title 1193 is: Zorba the Greek (Alexis Zorbas)\n",
    "        title 1194 is: tom thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 8\n",
    "\n",
    "**Time estimate:** One hour\n",
    "\n",
    "Next, we need to create the tf-idf matrix for the movie articles. Complete the `create_movie_matrix` function below. This code should closely match the code in Section 4. \n",
    "\n",
    "Based on my tests, I would like you to change one detail of the code in section 4. I found it better to reduce the magnitude of tf-idf scores for very popular words. You can do this by telling sci-kit learn to use $tf = log(tf)$ by specifying:\n",
    "\n",
    "    TfidfTransformer(norm=\"l2\", sublinear_tf=True)\n",
    "    \n",
    "You can use the `test_movie_matrix` function below to test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def test_movie_matrix(path):\n",
    "    M = create_movie_matrix(path)\n",
    "    print('Shape is' + str(M.shape))\n",
    "    print('Selected entry is ' + str(M[0,60634]))\n",
    "    \n",
    "def create_movie_matrix(path):\n",
    "    \"\"\"Returns the tf-idf transformed movie feature matrix for the specified movies.txt.\"\"\"\n",
    "    \n",
    "#test_movie_matrix(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should output exactly:\n",
    "\n",
    "        Shape is(1195, 60920)\n",
    "        Selected entry is 0.0524889070596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 9.\n",
    "\n",
    "**Time estimate:** Two hours.\n",
    "\n",
    "Imagine you are designing a system that collects text documents from users and you would like to provide a visual \"gist\" for a document. One simple NLP technique for describing a text documcent is displaying the highest scoring terms in a document's tf-idf vector.\n",
    "\n",
    "Complete the top_terms function below. It should return the top n terms that have highest values in your tf-idf vector. ***Hint:*** You may find it easier to convert the sci-kit vector to a native python vector using your `sk_vector_to_simple_row` method.\n",
    "\n",
    "You can test your function using the `test_top_terms` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_terms(lexicon, vector, n):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    \"\"\"    \n",
    "\n",
    "def create_lexicon(path):\n",
    "    \"\"\"Utility function that returns a list that can be used to map from feature indexes to terms.\"\"\"\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit_transform(readMovieDocs(path))\n",
    "    return count_vectorizer.get_feature_names()\n",
    "    \n",
    "def test_top_terms(path):\n",
    "    lexicon = create_lexicon(path)\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path) \n",
    "    for (title, row) in zip(titles, matrix)[:20]:\n",
    "        terms = top_terms(lexicon, row, 10)\n",
    "        print('top terms for \"%s\" are: %s\\n' % (title, terms))\n",
    "\n",
    "#test_top_terms(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines of test_top_terms should display:\n",
    "\n",
    "        top terms for \"12 Years a Slave\" are: [u'northup', u'ejiofor', u'epps', u'nyong', u'slave', u'chiwetel', u'mcqueen', u'slavery', u'patsey', u'fassbender']\n",
    "        \n",
    "        top terms for \"20 Feet from Stardom\" are: [u'stardom', u'lawry', u't\\xe1ta', u'darlene', u'vega', u'fischer', u'86th', u'feet', u'clayton', u'merry']\n",
    "        \n",
    "You'll notice that these terms are extremely specific. Create a second version of the top_terms function that filters out any terms that appear in too few documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_terms2(lexicon, vector, n, df, min_docs):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    Any terms that appear in less than min_docs documents will be removed.\n",
    "    df is a dictionary whose keys are terms and values are the number of documents it appears in.\n",
    "    \"\"\"\n",
    "    \n",
    "def sk_document_freq(lexicon, matrix):\n",
    "    \"\"\"Utility method that returns a dict whose keys are terms and values are document frequencies.\"\"\"\n",
    "    df = defaultdict(int)\n",
    "    for row in matrix:\n",
    "        for i in row.indices:\n",
    "            term = lexicon[i]\n",
    "            df[term] += 1\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def test_top_terms2(path):\n",
    "    lexicon = create_lexicon(path)\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path)    \n",
    "    doc_freq = sk_document_freq(lexicon, matrix)\n",
    "    for (title, row) in zip(titles, matrix)[:20]:\n",
    "        terms = top_terms2(lexicon, row, 10, doc_freq, 20)\n",
    "        print('top terms for \"%s\" are: %s\\n' % (title, terms))\n",
    "\n",
    "#test_top_terms2(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this enhanced method, the terms for the first movie should be:\n",
    "\n",
    "        top terms for \"12 Years a Slave\" are: [u'slave', u'mcqueen', u'bass', u'12', u'ford', u'sailor', u'cotton', u'christian', u'historic', u'twelve']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 10.\n",
    "\n",
    "**Time estimate:** Two hours.\n",
    "\n",
    "For your final task, you'll calculate the most similar neighbor articles for a particular movie article. You should use the `sk_dot` function from Section 5 as your measure of similarity.\n",
    "\n",
    "Complete the neighbors function below. It should calculated the similarity to every other neighbor, and print out the top 10 similarity scores and movie titles in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors(titles, matrix, target):\n",
    "    \"\"\"\n",
    "        Given a target movie's sk vector, finds the 10 most similar other rows (i.e. movies).\n",
    "        Prints out the scores and titles for each of the most similar movies.\n",
    "    \"\"\"\n",
    "\n",
    "def test_neighbors(path):\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path)\n",
    "    target = matrix[9]   # the abyss\n",
    "    neighbors(titles, matrix, target)\n",
    "\n",
    "test_neighbors(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
