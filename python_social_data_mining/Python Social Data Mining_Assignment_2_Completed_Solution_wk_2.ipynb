{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Practice Exercise 1.\n",
    "\n",
    "**Time estimate:** 20 minutes\n",
    "\n",
    "Study the `make_tf_vector()` function in Section 1 (reproduced below). Answer the three questions below. You do not need to write code... just explain in words.\n",
    "\n",
    "* What does `doc.split()` precisely do? \n",
    "* What are its limitations introduced to `make_tf_vector` when `split()`  is used like this? \n",
    "* How might you improve it? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1})]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def make_tf_vector(doc):\n",
    "    \"\"\"\n",
    "        Calculates the term frequency of a document.\n",
    "        Given a string, splits it into words on whitespace.\n",
    "        Returns a dictionary mapping words to their frequency.\n",
    "    \"\"\"\n",
    "    v = defaultdict(int)\n",
    "    for term in doc.split():\n",
    "        v[term] += 1\n",
    "    return v\n",
    "\n",
    "tf_matrix = []\n",
    "for doc in docs:    \n",
    "    v = make_tf_vector(doc)    \n",
    "    tf_matrix.append(v)\n",
    "\n",
    "pprint(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer to practice exercise 1:\n",
    "\n",
    "`doc.split()` breaks up the document (a String) based on whitespace (newlines, spaces, tabs, etc.). The function contains a list returning the individual tokens (i.e. words) in the document string. More details, including the complete list of whitespace characters, can be found in the online [string.split Python documentation](http://docs.python.org/2/library/stdtypes.html#str.split).\n",
    "\n",
    "The `split()` method introduces several limitations. In particular, it will become confused by things like punctuation. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'world!']\n"
     ]
    }
   ],
   "source": [
    "print('Hello, world!'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first token (\"`Hello,`\") includes the trailing comma, and the last token (\"`world!`\") includes the exclamation point. We could work around this by writing a loop that more carefully analyzes the letters in a string against a predefined list of characters. Another option is to use the version of split that takes a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.split('[^a-zA-Z0-9]+', 'Hello, world!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression above splits on non-alphanumeric characters. The '[^]' represents \"characters that are not any of the following...\". The '+' at the end means multiple consecutive non-alphanumeric characters should be collapsed together. However, **the best choice is to use nltk.** (This is internally used by scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n",
      "['Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "def has_letter(s):\n",
    "    \"\"\"Returns true iff the string s contains at least one letter.\"\"\"\n",
    "    return any(c.isalpha() for c in t)\n",
    "\n",
    "tokens_and_punc = wordpunct_tokenize('Hello, world!')\n",
    "tokens = [t for t in tokens_and_punc if has_letter(t)]   # filter out tokens that are just punctuation\n",
    "\n",
    "print(tokens_and_punc)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Practice Exercise 2.\n",
    "\n",
    "**Time estimate:** 20 minutes.\n",
    "\n",
    "The Python code for the `l2norm()` function in Section 2 (shown below) uses a [list comprehension](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html). Rewrite the l2norm function using a standard \"for-each\" loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}), 'is', 3.4641016151377544)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}), 'is', 3.1622776601683795)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1}), 'is', 2.449489742783178)\n"
     ]
    }
   ],
   "source": [
    "def l2norm(tf_vector):\n",
    "    \"\"\"\n",
    "        Returns the l2-norm (i.e. Euclidean length) of a vector.\n",
    "    \"\"\"\n",
    "    return sum([x*x for x in tf_vector.values()]) ** 0.5\n",
    "\n",
    "for tf_vector in tf_matrix:\n",
    "    norm = l2norm(tf_vector)\n",
    "    print('l2norm of' , tf_vector, 'is', norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer to Practice Exercise 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}), 'is', 3.4641016151377544)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}), 'is', 3.1622776601683795)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1}), 'is', 2.449489742783178)\n"
     ]
    }
   ],
   "source": [
    "def l2norm(tf_vector):\n",
    "    \"\"\"\n",
    "        Returns the l2-norm (i.e. Euclidean length) of a vector.\n",
    "    \"\"\"\n",
    "    sum_sq = 0.0\n",
    "    for x in tf_vector.values():\n",
    "        sum_sq += x * x\n",
    "    return sum_sq ** 0.5\n",
    "\n",
    "for tf_vector in tf_matrix:\n",
    "    norm = l2norm(tf_vector)\n",
    "    print('l2norm of' , tf_vector, 'is', norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 1: \n",
    "\n",
    "**Time estimate:** 20 minutes\n",
    "\n",
    "The Python code for the normalize function (below) uses a list comprehension. Rewrite the normalize function using a standard \"for-each\" loop. Check your answer against the results posted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
      "3.46410161514\n",
      "{'me': 0.5773502691896258, 'Julie': 0.2886751345948129, 'loves': 0.5773502691896258, 'Linda': 0.2886751345948129, 'than': 0.2886751345948129, 'more': 0.2886751345948129}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "        Given a dictionary representing a sparse vector, returns a new rescaled vector.\n",
    "        The new vector contains values from the original vector rescaled by a constant.\n",
    "        The new vector will have an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    norm = l2norm(vector)\n",
    "    if norm == 0.0:\n",
    "        return dict(vector)\n",
    "    \n",
    "    return dict([(term, tf/norm) for (term, tf) in vector.items()])\n",
    "\n",
    "v = {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
    "print(v)\n",
    "print(l2norm(v))\n",
    "print(normalize(v))\n",
    "print(l2norm(normalize(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer to Question 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
      "3.46410161514\n",
      "{'me': 0.5773502691896258, 'Julie': 0.2886751345948129, 'loves': 0.5773502691896258, 'Linda': 0.2886751345948129, 'than': 0.2886751345948129, 'more': 0.2886751345948129}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "        Given a dictionary representing a sparse vector, returns a new rescaled vector.\n",
    "        The new vector contains values from the original vector rescaled by a constant.\n",
    "        The new vector will have an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    l2 = l2norm(vector)\n",
    "    if l2 == 0.0:\n",
    "        return dict(vector)\n",
    "\n",
    "    norm = {}\n",
    "    for term in vector:\n",
    "        norm[term] = vector[term] / l2\n",
    "    return norm\n",
    "\n",
    "v = {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
    "print(v)\n",
    "print(l2norm(v))\n",
    "print(normalize(v))\n",
    "print(l2norm(normalize(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 2:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Answer two questions about the `make_doc_frequency` function (below):\n",
    "\n",
    "1. Notice the use of `set()` in the inner-most for loop. Why is this necessary? How (specifically) would the result change if we didn't use it?\n",
    "2. The performance of `make_doc_frequency` is linear, but linear *in what?*  Does the performance of this method scale with the number of documents, the number of unique terms, or total word count across all documents? Explain your answer. Hint: All set operations require constant time, as do dictionary sets and gets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'me': 2, 'basketball': 1, 'Julie': 2, 'baseball': 1, 'likes': 2, 'loves': 2, 'Jane': 1, 'Linda': 1, 'He': 1, 'than': 3, 'more': 3})\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_doc_frequency(docs):\n",
    "    \"\"\"\n",
    "        Given a collection of documents (Strings), \n",
    "        returns a dictionary mapping each word to the number of times it appears.        \n",
    "        Words are split on whitespace.\n",
    "    \"\"\"\n",
    "    df = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for term in set(d.split()):\n",
    "            df[term] += 1\n",
    "    return df\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "print(make_doc_frequency(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer for question 2:\n",
    "\n",
    "**Part 1:** If the function did not include the call to `set()`, a token (i.e. word) that appears in a single document more than once would be overcounted. For example, the term `me` appears twice in both the first and second sentence. Thus, the document frequency dict would have a value of 4 for me (2 + 2 + 0). `Love` would change to have a value of 3 (2 + 1 + 0).\n",
    "\n",
    "**Part 2:** The innermost for loop above runs once for every **distinct word** in every document. If \"the\" appears six times in a single document, the innermost \"for loop\" will only run once because a set keeps track of unique items. Since the amount of work in each loop is constant (a dictionary lookup and store, and an arithmetic addition), this suggests the function's complexity should scale with:\n",
    "\n",
    "    sum across all documents d of (number of unique words in d)\n",
    "\n",
    "However, this is not the whole story. Notice the `set()` function takes as a parameter all the words in the document, with words possibly appearing multiple times. Each of these words (including duplicates) must be added to the set. Therefore, the correct complexity is:\n",
    "\n",
    "    sum across all documents d of (length in words in d)\n",
    "    \n",
    "Or \"the total word count across all documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 3:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Complete the `make_tf_idf_vector` function below. The steps you need to complete are outlined in the comments for the method. Once you correctly complete the function, and run the test that appears below it, you should see the following output:\n",
    "\n",
    "        Julie loves me more than Linda loves me\n",
    "                  Linda: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=2 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        Jane likes me more than Julie loves me\n",
    "                   Jane: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        He likes basketball more than baseball\n",
    "             basketball: tf=1 tf-idf=+0.500\n",
    "               baseball: tf=1 tf-idf=+0.500\n",
    "                     He: tf=1 tf-idf=+0.500\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.354\n",
    "                   more: tf=1 tf-idf=-0.354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julie loves me more than Linda loves me\n",
      "\n",
      "Jane likes me more than Julie loves me\n",
      "\n",
      "He likes basketball more than baseball\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_tf_idf_vector(td, df, doc):\n",
    "    \"\"\"\n",
    "        Given a total document count (td), document frequency dictionary (words -> # docs), and a document (a string)\n",
    "        Returns a tf_idf vector.\n",
    "        The returned vector is normalized so that it has an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    # step 1: Calculate the tf vector\n",
    "    # step 2: Translate the tf vector into a tf-idf vector using the formula above\n",
    "    # step 3: Normalize the tf-idf vector so it has an l2 norm of 1.0\n",
    "    # step 4: return the normalized tf-idf vector.\n",
    "    return {}\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "td = len(docs)\n",
    "df = make_doc_frequency(docs)\n",
    "\n",
    "for d in docs:\n",
    "    tf = make_tf_vector(d)\n",
    "    tf_idf = make_tf_idf_vector(td, df, d)\n",
    "    print(d)\n",
    "    for term in sorted(tf_idf, key=tf_idf.get, reverse=True):\n",
    "        print('%15s: tf=%d tf-idf=%+.3f' % (term, tf[term], tf_idf[term]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer for question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julie loves me more than Linda loves me\n",
      "          Linda: tf=1 tf-idf=+0.706\n",
      "             me: tf=2 tf-idf=+0.000\n",
      "          Julie: tf=1 tf-idf=+0.000\n",
      "          loves: tf=2 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.501\n",
      "           more: tf=1 tf-idf=-0.501\n",
      "\n",
      "Jane likes me more than Julie loves me\n",
      "           Jane: tf=1 tf-idf=+0.706\n",
      "             me: tf=2 tf-idf=+0.000\n",
      "          Julie: tf=1 tf-idf=+0.000\n",
      "          likes: tf=1 tf-idf=+0.000\n",
      "          loves: tf=1 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.501\n",
      "           more: tf=1 tf-idf=-0.501\n",
      "\n",
      "He likes basketball more than baseball\n",
      "     basketball: tf=1 tf-idf=+0.500\n",
      "       baseball: tf=1 tf-idf=+0.500\n",
      "             He: tf=1 tf-idf=+0.500\n",
      "          likes: tf=1 tf-idf=+0.000\n",
      "           than: tf=1 tf-idf=-0.354\n",
      "           more: tf=1 tf-idf=-0.354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_tf_idf_vector(td, df, doc):\n",
    "    \"\"\"\n",
    "        Given a total document count (td), document frequency dictionary (words -> # docs), and a document (a string)\n",
    "        Returns a tf_idf vector.\n",
    "        The returned vector is normalized so that it has an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    # step 1: Calculate the tf vector\n",
    "    v = make_tf_vector(doc)\n",
    "\n",
    "    # step 2: Translate the tf vector into a tf-idf vector using the formula above\n",
    "    for term in v:\n",
    "        v[term] = v[term] * log(1.0 * td / (1 + df[term]))\n",
    "        \n",
    "    # step 3: Normalize the tf-idf vector so it has an l2 norm of 1.0\n",
    "    # step 4: return the normalized tf-idf vector.    \n",
    "    return normalize(v)\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "td = len(docs)\n",
    "df = make_doc_frequency(docs)\n",
    "\n",
    "for d in docs:\n",
    "    tf = make_tf_vector(d)\n",
    "    tf_idf = make_tf_idf_vector(td, df, d)\n",
    "    print(d)\n",
    "    for term in sorted(tf_idf, key=tf_idf.get, reverse=True):\n",
    "        print('%15s: tf=%d tf-idf=%+.3f' % (term, tf[term], tf_idf[term]))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 4:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Write a function called `sk_vector_to_simple_row` that returns a traditional (native) sparse vector representation for a row in a sci-kit learn matrix. Recall that the traditional python representation is a dict whose keys are terms and values are frequencies. Pattern your function after the example code in Section 4.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def sk_vector_to_simple_row(lexicon, row):\n",
    "    \"\"\"\n",
    "    Given a sci-kit learn row vector, return a native Python sparse vector.\n",
    "    The result will be a dictionary whose keys are terms and values are term frequencies.\n",
    "    Pattern this function after the code in Section 4.2.\n",
    "    \"\"\"\n",
    "\n",
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(docs)\n",
    "lexicon =  count_vectorizer.get_feature_names()\n",
    "sk_tf_matrix = count_vectorizer.transform(docs)\n",
    "\n",
    "for row in sk_tf_matrix:\n",
    "    print(sk_vector_to_simple_row(lexicon, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer for Question 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'me': 2, u'julie': 1, u'loves': 2, u'linda': 1, u'than': 1, u'more': 1}\n",
      "{u'me': 2, u'julie': 1, u'likes': 1, u'loves': 1, u'jane': 1, u'than': 1, u'more': 1}\n",
      "{u'basketball': 1, u'baseball': 1, u'likes': 1, u'he': 1, u'than': 1, u'more': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(docs)\n",
    "lexicon =  count_vectorizer.get_feature_names()\n",
    "sk_tf_matrix = count_vectorizer.transform(docs)\n",
    "\n",
    "def sk_vector_to_simple_row(lexicon, row):\n",
    "    \"\"\"\n",
    "    Given a sci-kit learn row vector, return a native Python sparse vector.\n",
    "    The result will be a dictionary whose keys are terms and values are term frequencies.\n",
    "    Pattern this function after the code in Section 4.2.\n",
    "    \"\"\"\n",
    "    simple = {}\n",
    "    for i in range(len(row.data)):\n",
    "        index = row.indices[i]    \n",
    "        term = lexicon[index]\n",
    "        val = row.data[i]\n",
    "        simple[term] = val\n",
    "    return simple\n",
    "\n",
    "for row in sk_tf_matrix:\n",
    "    print(sk_vector_to_simple_row(lexicon, row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 5:\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Study the code at the end of section 5 (measuring vector similarity). Notice that \"by hand\" we calculate the result of the `sk_dot` function between each pair of documents.\n",
    "\n",
    "Write a function that calculates the similarity between each pair of documents using for loops. The output of this function should look approximately like:\n",
    "\n",
    "    Similarity between document 0 and 1 is 0.753602532225\n",
    "    Similarity between document 0 and 2 is 0.128408027002\n",
    "    Similarity between document 1 and 2 is 0.128408027002\n",
    "    \n",
    "Use the following bit of python code to setup the sci-kit tf-idf matrix you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Train a tf-idf transformer on the dataset\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(sk_tf_matrix)\n",
    "sk_tf_idf_matrix = tfidf.transform(sk_tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:** The `shape` attribute of a matrix returns a two-tuple of the number of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n"
     ]
    }
   ],
   "source": [
    "print(sk_tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Answer for Question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between document 1 and 0 is 0.75360\n",
      "Similarity between document 2 and 0 is 0.12841\n",
      "Similarity between document 2 and 1 is 0.25742\n"
     ]
    }
   ],
   "source": [
    "def sk_dot(v1, v2):\n",
    "    return v1.dot(v2.transpose())[0,0]\n",
    "def pairwise(matrix):\n",
    "    nrows = matrix.shape[0]\n",
    "    for i in range(nrows):\n",
    "        for j in range(0, i):\n",
    "            vi = matrix[i]\n",
    "            vj = matrix[j]\n",
    "            sim = sk_dot(vi, vj)\n",
    "            print('Similarity between document %d and %d is %.5f' % (i, j, sim))\n",
    "\n",
    "pairwise(sk_tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 6.\n",
    "\n",
    "**Time estimate:** One hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For questions 6, 7, and 8 you will use a dataset I collected from Wikipedia that contains the article text for all the [Academy Award Winning Films on Wikipedia](http://en.wikipedia.org/wiki/List_of_Academy_Award-winning_films). After next week, you'll know how to collect this data yourself!\n",
    "\n",
    "The dataset is available on the course website called award_winners.zip. If you extract this file, you'll get a file called \"award_winners.txt.\" The format of the dataset is tab delimited, where the \"text\" field captures the extracted text of the Wikipedia article associated with the movie.\n",
    "\n",
    "    movie_title1       url1           text1\n",
    "    movie_title2       url2           text2\n",
    "    movie_title3       url3           text3    \n",
    "    .......\n",
    "    \n",
    "First, complete the generator function called readMovies below (we learned about generators last week). Each record generated by the file should be a dictionary with three keys: 'title', 'url', 'id', and 'text'. The ids should be assigned consecutive, and start with 0.\n",
    "\n",
    "You can use the `testReadMovies` function below to test your readMovies code. You should **not need to alter it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMovies(path):\n",
    "    \"\"\"\n",
    "    Returns a generator that yields a record for each movie in the specified movie.txt.\n",
    "    The format of a single record is {\n",
    "        'id' : 0,\n",
    "        'title' : '12 Years a Slave',\n",
    "        'url' : 'http://en.wikipedia.org/wiki/12_Years_a_Slave_(film)',\n",
    "        'text' : '12 Years a Slave is a 2013 British-Amer...'        \n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "def testReadMovies(path):\n",
    "    \"\"\"Prints debugging information about the movies.txt\"\"\"\n",
    "    for movie in readMovies(path):\n",
    "        if movie['id'] <= 3 or movie['id'] >= 1193:                \n",
    "            print(\"============================ Movie %d ============================\" % movie['id'])\n",
    "            print(\"'%s'\" % movie['title'])\n",
    "            print(movie['url'])\n",
    "            print(movie['text'][:80] + '...')\n",
    "\n",
    "#testReadMovies('movies.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you successfully complete The output from running your should be **exactly**:\n",
    "\n",
    "    ============================ Movie 0 ============================\n",
    "    '12 Years a Slave'\n",
    "    http://en.wikipedia.org/wiki/12_Years_a_Slave_(film)\n",
    "    12 Years a Slave is a 2013 British-American historical drama film and an adaptat...\n",
    "    ============================ Movie 1 ============================\n",
    "    '20 Feet from Stardom'\n",
    "    http://en.wikipedia.org/wiki/20_Feet_from_Stardom\n",
    "    20 Feet from Stardom is an Oscar-winning 2013 American documentary film directed...\n",
    "    ============================ Movie 2 ============================\n",
    "    '20,000 Leagues Under the Sea'\n",
    "    http://en.wikipedia.org/wiki/20,000_Leagues_Under_the_Sea_(1954_film)\n",
    "    20,000 Leagues Under the Sea is a 1954 American adventure film starring Kirk Dou...\n",
    "    ============================ Movie 3 ============================\n",
    "    '2001: A Space Odyssey'\n",
    "    http://en.wikipedia.org/wiki/2001:_A_Space_Odyssey_(film)\n",
    "    2001: A Space Odyssey is a 1968 British-American science fiction film produced a...\n",
    "    ============================ Movie 1193 ============================\n",
    "    'Zorba the Greek (Alexis Zorbas)'\n",
    "    http://en.wikipedia.org/wiki/Zorba_the_Greek_(film)\n",
    "    Zorba the Greek (Greek title: Αλέξης Ζορμπάς, Alexis Zorba(s))is a ...\n",
    "    ============================ Movie 1194 ============================\n",
    "    'tom thumb'\n",
    "    http://en.wikipedia.org/wiki/Tom_thumb_(film)\n",
    "    Tom Thumb (stylised as tom thumb) is a 1958 fantasy-musical film directed by Geo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Solution for Question 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'movies.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-bd98b334c092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'title'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'url'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtestReadMovies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'movies.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-d5998085439d>\u001b[0m in \u001b[0;36mtestReadMovies\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtestReadMovies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;34m\"\"\"Prints debugging information about the movies.txt\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mmovie\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreadMovies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmovie\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmovie\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1193\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"============================ Movie %d ============================\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmovie\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-bd98b334c092>\u001b[0m in \u001b[0;36mreadMovies\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mreadMovies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'title'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'url'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'movies.txt'"
     ]
    }
   ],
   "source": [
    "def readMovies(path):\n",
    "    for i, line in enumerate(open(path)):\n",
    "        tokens = line.split('\\t')\n",
    "        yield { 'title' : tokens[0], 'url' : tokens[1], 'text' : tokens[2].strip(), 'id' : i }\n",
    "\n",
    "testReadMovies('movies.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 7.\n",
    "\n",
    "**Time estimate:** 30 minutes.\n",
    "\n",
    "Next, complete the `readMovieDocs()` function below, which  returns a list of all the movie documents, in order. Also complete the `readMovieTitles()` that does the same thing for titles. You can use the `testReadDocsAndTitles` function below to make sure they are working properly.\n",
    "\n",
    "***Hint:*** You should make good use of your `readMovies` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readMovieDocs(path):\n",
    "    \"\"\"Returns a list of strings representing the movie articles text in the specified movies.txt\"\"\"\n",
    "\n",
    "def readMovieTitles(path):\n",
    "    \"\"\"Returns a list of strings representing the movie titles in the specified movies.txt\"\"\"\n",
    "    \n",
    "def testReadDocsAndTitles(path):\n",
    "    docs = readMovieDocs(path)\n",
    "    for i, d in enumerate(docs):\n",
    "        if i <= 3 or i >= 1193:          \n",
    "            print('doc %d is: %s' % (i, d[:50]))\n",
    "    print('\\n')\n",
    "            \n",
    "    titles = readMovieTitles(path)\n",
    "    for i, t in enumerate(titles):\n",
    "        if i <= 3 or i >= 1193:          \n",
    "            print('title %d is: %s' % (i, t))\n",
    "\n",
    "#testReadDocsAndTitles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should output exactly:\n",
    "\n",
    "        doc 0 is: 12 Years a Slave is a 2013 British-American histor\n",
    "        doc 1 is: 20 Feet from Stardom is an Oscar-winning 2013 Amer\n",
    "        doc 2 is: 20,000 Leagues Under the Sea is a 1954 American ad\n",
    "        doc 3 is: 2001: A Space Odyssey is a 1968 British-American s\n",
    "        doc 1193 is: Zorba the Greek (Greek title: Αλέξης Ζορ�\n",
    "        doc 1194 is: Tom Thumb (stylised as tom thumb) is a 1958 fantas\n",
    "        \n",
    "        \n",
    "        title 0 is: 12 Years a Slave\n",
    "        title 1 is: 20 Feet from Stardom\n",
    "        title 2 is: 20,000 Leagues Under the Sea\n",
    "        title 3 is: 2001: A Space Odyssey\n",
    "        title 1193 is: Zorba the Greek (Alexis Zorbas)\n",
    "        title 1194 is: tom thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Solution to Question 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 is: 12 Years a Slave is a 2013 British-American histor\n",
      "doc 1 is: 20 Feet from Stardom is an Oscar-winning 2013 Amer\n",
      "doc 2 is: 20,000 Leagues Under the Sea is a 1954 American ad\n",
      "doc 3 is: 2001: A Space Odyssey is a 1968 British-American s\n",
      "doc 1193 is: Zorba the Greek (Greek title: Αλέξης Ζορ�\n",
      "doc 1194 is: Tom Thumb (stylised as tom thumb) is a 1958 fantas\n",
      "\n",
      "\n",
      "title 0 is: 12 Years a Slave\n",
      "title 1 is: 20 Feet from Stardom\n",
      "title 2 is: 20,000 Leagues Under the Sea\n",
      "title 3 is: 2001: A Space Odyssey\n",
      "title 1193 is: Zorba the Greek (Alexis Zorbas)\n",
      "title 1194 is: tom thumb\n"
     ]
    }
   ],
   "source": [
    "def readMovieDocs(path):\n",
    "    return [m['text'] for m in readMovies(path)]\n",
    "\n",
    "def readMovieTitles(path):\n",
    "    return [m['title'] for m in readMovies(path)]\n",
    "\n",
    "testReadDocsAndTitles(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 8\n",
    "\n",
    "**Time estimate:** One hour\n",
    "\n",
    "Next, we need to create the tf-idf matrix for the movie articles. Complete the `create_movie_matrix` function below. This code should closely match the code in Section 4. \n",
    "\n",
    "Based on my tests, I would like you to change one detail of the code in section 4. I found it better to reduce the magnitude of tf-idf scores for very popular words. You can do this by telling sci-kit learn to use $tf = log(tf)$ by specifying:\n",
    "\n",
    "    TfidfTransformer(norm=\"l2\", sublinear_tf=True)\n",
    "    \n",
    "You can use the `test_movie_matrix` function below to test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def test_movie_matrix(path):\n",
    "    M = create_movie_matrix(path)\n",
    "    print('Shape is' + str(M.shape))\n",
    "    print('Selected entry is ' + str(M[0,60634]))\n",
    "    \n",
    "def create_movie_matrix(path):\n",
    "    \"\"\"Returns the tf-idf transformed movie feature matrix for the specified movies.txt.\"\"\"\n",
    "    \n",
    "#test_movie_matrix(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should output exactly:\n",
    "\n",
    "        Shape is(1195, 60920)\n",
    "        Selected entry is 0.0524889070596"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Solution to Question 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is(1195, 60920)\n",
      "Selected entry is 0.0524889070596\n"
     ]
    }
   ],
   "source": [
    "def create_movie_matrix(path):\n",
    "    \n",
    "    docs = readMovieDocs(path)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit_transform(docs)\n",
    "    lexicon =  count_vectorizer.get_feature_names()\n",
    "    sk_tf_matrix = count_vectorizer.transform(docs)\n",
    "    \n",
    "    tfidf = TfidfTransformer(norm=\"l2\", sublinear_tf=True)\n",
    "    tfidf.fit(sk_tf_matrix)        \n",
    "    return tfidf.transform(sk_tf_matrix)\n",
    "\n",
    "test_movie_matrix(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 9.\n",
    "\n",
    "**Time estimate:** Two hours.\n",
    "\n",
    "Imagine you are designing a system that collects text documents from users and you would like to provide a visual \"gist\" for a document. One simple NLP technique for describing a text documcent is displaying the highest scoring terms in a document's tf-idf vector.\n",
    "\n",
    "Complete the top_terms function below. It should return the top n terms that have highest values in your tf-idf vector. ***Hint:*** You may find it easier to convert the sci-kit vector to a native python vector using your `sk_vector_to_simple_row` method.\n",
    "\n",
    "You can test your function using the `test_top_terms` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_terms(lexicon, vector, n):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    \"\"\"    \n",
    "\n",
    "def create_lexicon(path):\n",
    "    \"\"\"Utility function that returns a list that can be used to map from feature indexes to terms.\"\"\"\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    count_vectorizer.fit_transform(readMovieDocs(path))\n",
    "    return count_vectorizer.get_feature_names()\n",
    "    \n",
    "def test_top_terms(path):\n",
    "    lexicon = create_lexicon(path)\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path) \n",
    "    for (title, row) in zip(titles, matrix)[:20]:\n",
    "        terms = top_terms(lexicon, row, 10)\n",
    "        print('top terms for \"%s\" are: %s\\n' % (title, terms))\n",
    "\n",
    "#test_top_terms(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines of test_top_terms should display:\n",
    "\n",
    "        top terms for \"12 Years a Slave\" are: [u'northup', u'ejiofor', u'epps', u'nyong', u'slave', u'chiwetel', u'mcqueen', u'slavery', u'patsey', u'fassbender']\n",
    "        \n",
    "        top terms for \"20 Feet from Stardom\" are: [u'stardom', u'lawry', u't\\xe1ta', u'darlene', u'vega', u'fischer', u'86th', u'feet', u'clayton', u'merry']\n",
    "        \n",
    "You'll notice that these terms are extremely specific. Create a second version of the top_terms function that filters out any terms that appear in too few documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def top_terms2(lexicon, vector, n, df, min_docs):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    Any terms that appear in less than min_docs documents will be removed.\n",
    "    df is a dictionary whose keys are terms and values are the number of documents it appears in.\n",
    "    \"\"\"\n",
    "    \n",
    "def sk_document_freq(lexicon, matrix):\n",
    "    \"\"\"Utility method that returns a dict whose keys are terms and values are document frequencies.\"\"\"\n",
    "    df = defaultdict(int)\n",
    "    for row in matrix:\n",
    "        for i in row.indices:\n",
    "            term = lexicon[i]\n",
    "            df[term] += 1\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def test_top_terms2(path):\n",
    "    lexicon = create_lexicon(path)\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path)    \n",
    "    doc_freq = sk_document_freq(lexicon, matrix)\n",
    "    for (title, row) in zip(titles, matrix)[:20]:\n",
    "        terms = top_terms2(lexicon, row, 10, doc_freq, 20)\n",
    "        print('top terms for \"%s\" are: %s\\n' % (title, terms))\n",
    "\n",
    "#test_top_terms2(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this enhanced method, the terms for the first movie should be:\n",
    "\n",
    "        top terms for \"12 Years a Slave\" are: [u'slave', u'mcqueen', u'bass', u'12', u'ford', u'sailor', u'cotton', u'christian', u'historic', u'twelve']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Solution for Question 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms for \"12 Years a Slave\" are: [u'northup', u'ejiofor', u'epps', u'nyong', u'slave', u'chiwetel', u'mcqueen', u'slavery', u'patsey', u'fassbender']\n",
      "\n",
      "top terms for \"20 Feet from Stardom\" are: [u'stardom', u'lawry', u't\\xe1ta', u'darlene', u'vega', u'fischer', u'86th', u'feet', u'clayton', u'merry']\n",
      "\n",
      "top terms for \"20,000 Leagues Under the Sea\" are: [u'aronnax', u'nautilus', u'nemo', u'conseil', u'leagues', u'mcg', u'ned', u'squid', u'vulcania', u'submarine']\n",
      "\n",
      "top terms for \"2001: A Space Odyssey\" are: [u'monolith', u'kubrick', u'bowman', u'poole', u'odyssey', u'jupiter', u'clarke', u'9000', u'millimetre', u'weidner']\n",
      "\n",
      "top terms for \"7 Faces of Dr. Lao\" are: [u'lao', u'randall', u'stark', u'lindquist', u'cassan', u'woldercan', u'circus', u'cunningham', u'serpent', u'medusa']\n",
      "\n",
      "top terms for \"7th Heaven\" are: [u'7th', u'borzage', u'heaven', u'gobin', u'gaynor', u'zhou', u'yuan', u'glazer', u'chico', u'1937']\n",
      "\n",
      "top terms for \"8 Mile\" are: [u'eminem', u'mile', u'basinger', u'wink', u'rap', u'rapper', u'jimmy', u'mekhi', u'phifer', u'greenfield']\n",
      "\n",
      "top terms for \"A Herb Alpert and the Tijuana Brass Double Feature\" are: [u'tijuana', u'alpert', u'brass', u'herb', u'hubley', u'prototypical', u'scribner', u'moonbird', u'double', u'flea']\n",
      "\n",
      "top terms for \"A Separation\" are: [u'nader', u'razieh', u'simin', u'hodjat', u'farhadi', u'termeh', u'iranian', u'separation', u'miscarriage', u'asghar']\n",
      "\n",
      "top terms for \"Abyss, TheThe Abyss\" are: [u'abyss', u'lindsey', u'coffey', u'ntis', u'mastrantonio', u'bud', u'nti', u'cameron', u'platform', u'diving']\n",
      "\n",
      "top terms for \"Accidental Tourist, TheThe Accidental Tourist\" are: [u'macon', u'muriel', u'leary', u'geena', u'sarah', u'pritchett', u'kasdan', u'kathleen', u'hurt', u'davis']\n",
      "\n",
      "top terms for \"Accountant, TheThe Accountant\" are: [u'dell', u'accountant', u'mckinnon', u'goggins', u'sakai', u'conspiracies', u'farms', u'mathematical', u'74th', u'plight']\n",
      "\n",
      "top terms for \"Accused, TheThe Accused\" are: [u'mcgillis', u'schock', u'rapists', u'kathryn', u'rape', u'sarah', u'coulson', u'tobias', u'jodie', u'raped']\n",
      "\n",
      "top terms for \"Adaptation\" are: [u'orlean', u'laroche', u'orchid', u'kaufman', u'jonze', u'streep', u'mckee', u'thief', u'charlie', u'swamp']\n",
      "\n",
      "top terms for \"Adventures of Don Juan\" are: [u'juan', u'flynn', u'errol', u'lorca', u'polan', u'lindfors', u'viveca', u'hale', u'eddington', u'adventures']\n",
      "\n",
      "top terms for \"Adventures of Priscilla, Queen of the Desert, TheThe Adventures of Priscilla, Queen of the Desert\" are: [u'priscilla', u'tick', u'bernadette', u'outback', u'felicia', u'drag', u'australian', u'stephan', u'mitzi', u'queens']\n",
      "\n",
      "top terms for \"Adventures of Robin Hood, TheThe Adventures of Robin Hood\" are: [u'gisbourne', u'marian', u'hood', u'robin', u'flynn', u'tournament', u'arrow', u'supporter', u'sherwood', u'errol']\n",
      "\n",
      "top terms for \"Affliction\" are: [u'whitehouse', u'wade', u'affliction', u'lariviere', u'margie', u'rolfe', u'jill', u'nolte', u'coburn', u'glen']\n",
      "\n",
      "top terms for \"African Queen, TheThe African Queen\" are: [u'louisa', u'bogart', u'torpedoes', u'african', u'rapids', u'queen', u'sayer', u'boiler', u'boat', u'charlie']\n",
      "\n",
      "top terms for \"Age of Innocence, TheThe Age of Innocence\" are: [u'archer', u'countess', u'scorsese', u'winona', u'wharton', u'ryder', u'pfeiffer', u'innocence', u'newland', u'olenska']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_terms(lexicon, vector, n):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    \"\"\"\n",
    "    simple = sk_vector_to_simple_row(lexicon, vector)    \n",
    "    terms = sorted(simple, key=simple.get, reverse=True)\n",
    "    return terms[:n]\n",
    "\n",
    "test_top_terms(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms for \"12 Years a Slave\" are: [u'slave', u'mcqueen', u'bass', u'12', u'ford', u'sailor', u'cotton', u'christian', u'historic', u'twelve']\n",
      "\n",
      "top terms for \"20 Feet from Stardom\" are: [u'feet', u'merry', u'judith', u'waters', u'sundance', u'singers', u'2013', u'jo', u'festival', u'20']\n",
      "\n",
      "top terms for \"20,000 Leagues Under the Sea\" are: [u'ned', u'sea', u'mason', u'monster', u'disney', u'albums', u'disneyland', u'kirk', u'1954', u'captain']\n",
      "\n",
      "top terms for \"2001: A Space Odyssey\" are: [u'kubrick', u'odyssey', u'clarke', u'hal', u'floyd', u'space', u'discovery', u'science', u'stanley', u'2001']\n",
      "\n",
      "top terms for \"7 Faces of Dr. Lao\" are: [u'stark', u'circus', u'angela', u'faces', u'mike', u'dr', u'henchmen', u'tony', u'cowboy', u'monster']\n",
      "\n",
      "top terms for \"7th Heaven\" are: [u'heaven', u'1937', u'diane', u'chinese', u'1927', u'angel', u'seventh', u'remake', u'china', u'janet']\n",
      "\n",
      "top terms for \"8 Mile\" are: [u'jimmy', u'rabbit', u'doc', u'yourself', u'alex', u'gang', u'stephanie', u'leaders', u'murphy', u'lose']\n",
      "\n",
      "top terms for \"A Herb Alpert and the Tijuana Brass Double Feature\" are: [u'herb', u'double', u'animated', u'soundtracks', u'feature', u'rod', u'taxi', u'animators', u'summary', u'cartoons']\n",
      "\n",
      "top terms for \"A Separation\" are: [u'bear', u'religious', u'2011', u'hires', u'silver', u'physically', u'divorce', u'poll', u'outraged', u'class']\n",
      "\n",
      "top terms for \"Abyss, TheThe Abyss\" are: [u'cameron', u'platform', u'underwater', u'tank', u'wave', u'breathing', u'sub', u'water', u'surface', u'dive']\n",
      "\n",
      "top terms for \"Accidental Tourist, TheThe Accidental Tourist\" are: [u'sarah', u'kathleen', u'hurt', u'davis', u'tourist', u'accidental', u'turner', u'1988', u'taxi', u'travel']\n",
      "\n",
      "top terms for \"Accountant, TheThe Accountant\" are: [u'walton', u'vanessa', u'richardson', u'corporate', u'explores', u'farmer', u'armed', u'skills', u'tommy', u'nick']\n",
      "\n",
      "top terms for \"Accused, TheThe Accused\" are: [u'rape', u'sarah', u'raped', u'accused', u'foster', u'attorney', u'murphy', u'bar', u'trial', u'joyce']\n",
      "\n",
      "top terms for \"Adaptation\" are: [u'thief', u'charlie', u'donald', u'cage', u'spike', u'cooper', u'susan', u'nicolas', u'cox', u'adaptation']\n",
      "\n",
      "top terms for \"Adventures of Don Juan\" are: [u'juan', u'adventures', u'duke', u'count', u'don', u'spanish', u'brent', u'queen', u'de', u'margaret']\n",
      "\n",
      "top terms for \"Adventures of Priscilla, Queen of the Desert, TheThe Adventures of Priscilla, Queen of the Desert\" are: [u'drag', u'australian', u'desert', u'adam', u'queen', u'bus', u'adventures', u'terence', u'sydney', u'marion']\n",
      "\n",
      "top terms for \"Adventures of Robin Hood, TheThe Adventures of Robin Hood\" are: [u'robin', u'prince', u'sir', u'adventures', u'olivia', u'bishop', u'sheriff', u'maid', u'bugs', u'basil']\n",
      "\n",
      "top terms for \"Affliction\" are: [u'wade', u'glen', u'nick', u'hunting', u'conviction', u'jack', u'divorced', u'custody', u'obsessed', u'policeman']\n",
      "\n",
      "top terms for \"African Queen, TheThe African Queen\" are: [u'african', u'queen', u'boat', u'charlie', u'rose', u'hepburn', u'africa', u'huston', u'lake', u'restoration']\n",
      "\n",
      "top terms for \"Age of Innocence, TheThe Age of Innocence\" are: [u'scorsese', u'innocence', u'michelle', u'ellen', u'lewis', u'age', u'martin', u'announces', u'daniel', u'engagement']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def top_terms2(lexicon, vector, n, df, min_docs):\n",
    "    \"\"\"\n",
    "    Given a sci-kit sparse tf-idf vector, returns a list of the highest-scoring n terms.\n",
    "    Any terms that appear in less than min_docs documents will be removed.\n",
    "    df is a dictionary whose keys are terms and values are the number of documents it appears in.\n",
    "    \"\"\"\n",
    "    simple = sk_vector_to_simple_row(lexicon, vector) \n",
    "    terms = sorted(simple, key=simple.get, reverse=True)\n",
    "    terms = [t for t in terms if df[t] >= min_docs]\n",
    "    return terms[:n]\n",
    "\n",
    "test_top_terms2(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment Question 10.\n",
    "\n",
    "**Time estimate:** Two hours.\n",
    "\n",
    "For your final task, you'll calculate the most similar neighbor articles for a particular movie article. You should use the `sk_dot` function from Section 5 as your measure of similarity.\n",
    "\n",
    "Complete the neighbors function below. It should calculated the similarity to every other neighbor, and print out the top 10 similarity scores and movie titles in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neighbors(titles, matrix, target):\n",
    "    \"\"\"\n",
    "        Given a target movie's sk vector, finds the 10 most similar other rows (i.e. movies).\n",
    "        Prints out the scores and titles for each of the most similar movies.\n",
    "    \"\"\"\n",
    "\n",
    "def test_neighbors(path):\n",
    "    titles = readMovieTitles(path)\n",
    "    matrix = create_movie_matrix(path)\n",
    "    target = matrix[9]   # the abyss\n",
    "    neighbors(titles, matrix, target)\n",
    "\n",
    "test_neighbors(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Solution to Question 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.99999999999999822, 'Abyss, TheThe Abyss')\n",
      "(0.24877942071530276, 'Titanic')\n",
      "(0.23415443073467382, 'Alien')\n",
      "(0.23348650601736651, 'Avatar')\n",
      "(0.23063808100837763, 'Jaws')\n",
      "(0.22794608039107156, 'Terminator 2: Judgment Day')\n",
      "(0.22553156006579073, '2001: A Space Odyssey')\n",
      "(0.21666064780549232, 'Gravity')\n",
      "(0.21602804890256305, 'Aliens')\n",
      "(0.21470730303397395, 'Independence Day')\n"
     ]
    }
   ],
   "source": [
    "def neighbors(titles, matrix, target):\n",
    "    \"\"\"\n",
    "        Given a target movies, finds the 10 most similar other rows (i.e. movies).\n",
    "        Prints out the scores and titles for each of the most similar movies.\n",
    "    \"\"\"\n",
    "    neighbors = []\n",
    "    for (i, candidate) in enumerate(matrix):\n",
    "        neighbors.append((sk_dot(target, candidate), titles[i]))\n",
    "    neighbors.sort()\n",
    "    neighbors.reverse()\n",
    "    for n in neighbors[:10]:\n",
    "        print(n)\n",
    "\n",
    "test_neighbors(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
