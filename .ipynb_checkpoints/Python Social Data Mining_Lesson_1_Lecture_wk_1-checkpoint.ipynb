{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Statistics.com: \"Social Data Mining\", Shilad Sen\n",
    "#Lesson 1: Managing data & recommenders\n",
    "\n",
    "Humans generate huge social datasets.\n",
    "Wikipedians have edited pages over half a billion times.\n",
    "Twitter users generate nearly a billion tweets every day.\n",
    "\n",
    "In this lesson you'll hone your Python data skills to efficiently analyze these datasets, and practice your craft in the context of basic recommender algorithms. More specifically, you will:\n",
    "\n",
    " * Prepare your Python development environment for the course.\n",
    " * Deepen your understanding of the four basic Python data structures: lists, dictionaries, sets, and tuples. Learn to select the appropriate data structures for a scenario.\n",
    " * Apply streaming techniques to analyze datasets that cannot fit in memory.\n",
    " * Use Python's `yield` keyword to ease streaming and code reuse.\n",
    " * Understand and implement association-based recommendation algorithms (if you liked ``The Matrix`` you'll like...).\n",
    " * Begin learning to evaluate the time complexity of your Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Python data structures\n",
    "\n",
    "This section of the lesson provides you with a refresher on the data structures you previously learned in Python. It should take about 45 minutes (not including the assignment). \n",
    "\n",
    "As with the other lessons, **I encourage you to follow along and type the examples into your own Python interpreter or ipython notebook.**\n",
    "\n",
    "###1.1. Lists\n",
    "Here's a short refresher on lists. Let's presume that you run a book website and are storing the numeric rating values for a particular book in a list called `ratings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "5.0\n",
      "[4.0, 2.0, 1.5]\n",
      "[1.0, 1.5, 2.0, 3.0, 3.5, 4.0, 5.0]\n",
      "2.85714285714\n",
      "('found a high rating:', 4.0)\n",
      "('found a high rating:', 5.0)\n"
     ]
    }
   ],
   "source": [
    "ratings = [3.5, 5.0, 2.0, 1.5, 3.0, 5.0]\n",
    "print(ratings[1])        # second element\n",
    "print(ratings[-1])       # last element\n",
    "ratings[1] = 4.0         # reassign second element\n",
    "ratings.append(1.0)      # add a low rating\n",
    "print(ratings[1:4])      # the second element through the fourth, inclusive\n",
    "ratings.sort()           # sort the list in place\n",
    "print(ratings)\n",
    "print(1.0 * sum(ratings) / len(ratings))   # the mean rating, \"1.0 *\" is required to convert the expression to a float in Python2.x\n",
    "\n",
    "# print out all high rating values\n",
    "for r in ratings: \n",
    "    if r >= 4.0: print('found a high rating:', r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like more information, the Python Programming Wikibook provides excellent supplementary tutorials on [lists](http://en.wikibooks.org/wiki/Python_Programming/Lists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.2. Dictionaries: \n",
    "Lists store values associated with a consecutive numeric index. Dictionaries, on the other hand, can store values associated with any key.   A dictionary only stores one value for every key. In other words, it cannot have duplicate keys. \n",
    "\n",
    "Let's presume that you want to enhance your website so that it doesn't just store the raw rating values. \n",
    "You would like to associate each rating with a particular person using their first name.\n",
    "For this case, we would create a dictionary whose **keys** are users' first names, and whose **values** are their ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "['Jane', 'Anne', 'Matt', 'Dave']\n",
      "<type 'list'>\n",
      "[1.5, 4.0, 3.5, 4.5]\n",
      "<type 'list'>\n",
      "4.0\n",
      "('found high rater:', 'Anne')\n",
      "('found high rater:', 'Dave')\n"
     ]
    }
   ],
   "source": [
    "person_ratings = { 'Anne' : 4.0, 'Matt' : 3.5, 'Dave' : 4.5, 'Jane' : 1.5}\n",
    "\n",
    "print(person_ratings['Matt'])    # get the value associated with a particular key\n",
    "print(person_ratings.keys())     # list of first names, can be iterated over using a for loop\n",
    "print type(person_ratings.keys())\n",
    "print(person_ratings.values())   # list of rating values, can be iterated over using a for loop.\n",
    "print type(person_ratings.values())\n",
    "# check whether a particular key exists.\n",
    "if 'Anne' in person_ratings:\n",
    "    print(person_ratings['Anne'])\n",
    "\n",
    "# loop over all people and print out the names of high raters.    \n",
    "for name in person_ratings:\n",
    "    if person_ratings[name] >= 4.0:\n",
    "        print('found high rater:', name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like more information, the Python Programming Wikibook provides excellent supplementary tutorials on [dictionaries](http://en.wikibooks.org/wiki/Python_Programming/Dictionaries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Tuples\n",
    "\n",
    "Tuples function very similarly to lists with one important exception. They are *immutable* - they cannot be changed once created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.5, 5.0, 2.0, 1.5, 3.0, 5.0)\n"
     ]
    }
   ],
   "source": [
    "ratings = (3.5, 5.0, 2.0, 1.5, 3.0, 5.0)   # notice the parenthesis instead of brackets\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.4. Sets\n",
    "\n",
    "Lists efficiently store an ordered collection of elements, but sometimes we need a collection that can quickly determine membership. (More about what \"quickly\" actually means later). A set supports this case efficiently. However, in exchange for this efficiency it does not support duplicate values, and does not remember the order in which things were added. \n",
    "\n",
    "To return to our example, presume we wanted to keep track of the first names of everybody who provides a rating of 1.5 or lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('low raters are', set(['Jane', 'Matt', 'Scott']))\n",
      "Matt is a low rater\n"
     ]
    }
   ],
   "source": [
    "book1_ratings = { 'Anne' : 4.0, 'Matt' : 3.5, 'Dave' : 4.5, 'Jane' : 1.5}\n",
    "book2_ratings = { 'Scott' : 1.0, 'Matt' : 1.5, 'Dave' : 4.0, 'Jane' : 1.0}\n",
    "\n",
    "low_raters = set()    # creates a new empty set\n",
    "\n",
    "for name in book1_ratings:\n",
    "    if book1_ratings[name] <= 1.5:\n",
    "        low_raters.add(name)\n",
    "        \n",
    "for name in book2_ratings:\n",
    "    if book2_ratings[name] <= 1.5:\n",
    "        low_raters.add(name)\n",
    "\n",
    "# Print out the results. Note that 'Jane' only appears once\n",
    "print('low raters are', low_raters)\n",
    "\n",
    "# check membership for a set using in.\n",
    "if 'Matt' in low_raters:\n",
    "    print('Matt is a low rater')\n",
    "else:\n",
    "    print('Matt is not a low rater')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like more information, the Python Programming Wikibook provides excellent supplementary tutorials on [sets](http://en.wikibooks.org/wiki/Python_Programming/Sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1.5. Nested data structures\n",
    "\n",
    "When performing data analysis, these three data structures are commonly combined to efficiently represent more sophisticated relationships.  For example, we could have included the `book1_ratings` and `book2_ratings` dictionaries in a single `book_ratings` dictionary keyed by book title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('low raters are', set(['Jane', 'Matt', 'Scott']))\n",
      "CPU times: user 48 µs, sys: 5 µs, total: 53 µs\n",
      "Wall time: 52.9 µs\n"
     ]
    }
   ],
   "source": [
    "all_book_ratings = {}  # empty dictionary\n",
    "all_book_ratings['Lean In'] = { 'Anne' : 4.0, 'Matt' : 3.5, 'Dave' : 4.5, 'Jane' : 1.5}\n",
    "all_book_ratings['Inferno'] = { 'Scott' : 1.0, 'Matt' : 1.5, 'Dave' : 4.0, 'Jane' : 1.0}\n",
    "\n",
    "low_raters = set()    # creates a new empty set\n",
    "\n",
    "for title in all_book_ratings:   # we don't use the book titles for now\n",
    "    book_ratings = all_book_ratings[title]   # get a single book's rating dictionary\n",
    "    for name in book_ratings:\n",
    "        if book_ratings[name] <= 1.5:\n",
    "            low_raters.add(name)\n",
    "\n",
    "# Print out the results. Note that 'Jane' only appears once\n",
    "print('low raters are', low_raters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like more details, the [Pasteur institute](http://www.pasteur.fr/) has developed a short tutorial describing [nested data structures](http://www.pasteur.fr/formation/infobio/python/ch10.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Data structure summary\n",
    "Watch the first 12 minutes of [Alex Gaynor's PyCon talk](http://pyvideo.org/video/420/pycon-2011--the-data-structures-of-python) to reinforce the concepts above. To recap, lists, dictionaries, and sets provide distinct features:\n",
    "\n",
    "Lists:\n",
    "\n",
    " * Stores values sequentially\n",
    " * Retains the ordering of elements added to it.\n",
    " * Allows duplicate values\n",
    " * Literals can be created using square brackets: `x = [3, 9, 7]`\n",
    " * New elements can be added using `append()`\n",
    " * Provides fast indexing (e.g. `ratings[1]`)\n",
    " \n",
    "Dictionaries:\n",
    "\n",
    " * Stores values associated with a unique key\n",
    " * Does not retain the ordering of keys added to it\n",
    " * Does not allow duplicate keys, but...\n",
    " * Allows duplicate values\n",
    " * Literals can be created using curly brackets: `x = {'a' : 3, 'q' : 9, 'c' : 7}`\n",
    " * New elements can be added using indexing: `x['z'] = 3`\n",
    " * Provides fast lookup by key (e.g. `x['q']`)\n",
    " \n",
    "Tuples:\n",
    "\n",
    " * Identical to lists, but they cannot be changed.\n",
    " * Literals can be created using parenthesis: `x = (3, 9, 7)`\n",
    " \n",
    "Sets:\n",
    "\n",
    " * Stores a collection of distinct values.\n",
    " * Does not allow duplicate values.\n",
    " * Does not retain the ordering of elements added to it.\n",
    " * Literals can be created by passing a list to the  set constructor: `x = set([3, 9, 4])`\n",
    " * New elements can be added using add `x.add(12)`.\n",
    " * Does not provide indexing, but provides fast membership checking via `in`\n",
    " \n",
    "**All four data structures support iteration using the `for` loop and membership checking using `in`.**\n",
    "\n",
    "Execution Time for Methods\n",
    "<table>\n",
    "<tr><td>Method/Data Structure</td><td>list</td><td>set</td></tr>\n",
    "<tr><td>append/add</td><td>O(1) </td><td>O(1)</td></tr>\n",
    "<tr><td>in, not in</td><td>O(1) </td><td>O(n)</td></tr>\n",
    "<tr><td>remove</td><td>O(1) </td><td>O(n)</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#2. Streaming large datasets\n",
    "\n",
    "This section of the lesson will show you how to analyze datasets that cannot fit in memory. It should take you approximately 30 minutes to complete this section.\n",
    "\n",
    "In this course, you'll stream datasets that cannot fit in memory at once. Typically, you'll create variables that accumulate quantities you're interested in and update these variables as you stream through the **records** in a file.\n",
    "\n",
    "We'll practice streaming using the MovieLens 10M dataset, which contains anonymized data from the [MovieLens movie recommender](http://movielens.org). The dataset we will analyze contains roughly ten million movie ratings. (As an aside, during graduate school I led the development of the tagging system whose data is included in this dataset). \n",
    "\n",
    "Download the [MovieLens 10M dataset](http://files.grouplens.org/datasets/movielens/ml-10m.zip) and extract the zip file on your computer. Note the path to the directory you've placed the file in, because you'll need it later.\n",
    "\n",
    "The data format for the files are detailed in the [ML-10M Readme](http://files.grouplens.org/datasets/movielens/ml-10m-README.html). We'll focus on ratings.dat. The file contains one movie rating per line. Each line has four `'::'` delimited fields: a numeric user id, movie id, and rating followed by a timestamp in [seconds since the epoch](http://en.wikipedia.org/wiki/Unix_time). An excerpt of the last 10 lines of the file follows:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "71567::1984::1::912580553\n",
    "71567::1985::1::912580553\n",
    "71567::1986::1::912580553\n",
    "71567::2012::3::912580722\n",
    "71567::2028::5::912580344\n",
    "71567::2107::1::912580553\n",
    "71567::2126::2::912649143\n",
    "71567::2294::5::912577968\n",
    "71567::2338::2::912578016\n",
    "71567::2384::2::912578173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Calculating the mean rating for MovieLens 10M\n",
    "\n",
    "Let's begin by simply calculating the mean rating value. A traditional stats tool such as R or JMP would probably read this dataset into in-memory tables and then iterate through them. While this may work on this dataset if you have a powerful computer, what happens if the dataset contains one hundred million or one billion records? You're out of luck!\n",
    "\n",
    "Luckily, we don't need to store the entire dataset into memory to calculate the mean. We only need two accumulator variables: the count of values and sum of values. We can then process the data file line by line, storing only one line in memory at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mean of', 10000054, 'ratings is', 3.512421932921562)\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat'  # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "# accumulators necessary for the mean\n",
    "rating_sum = 0.0\n",
    "rating_count = 0\n",
    "\n",
    "\n",
    "fp = open(path,\"r\")\n",
    "for line in fp:\n",
    "    tokens = line.split('::')    # tokens is a list of the four values in each record\n",
    "    rating = float(tokens[2])\n",
    "    rating_count += 1\n",
    "    rating_sum += rating\n",
    "\n",
    "print('mean of', rating_count, 'ratings is', (rating_sum / rating_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Calculating the histogram of rating for MovieLens 10M.\n",
    "\n",
    "Next, we'll try to calculate a histogram of the rating values.\n",
    "\n",
    "A dictionary naturally supports a histogram; the dictionary's keys are things you are counting (rating values) are the dictionary's values are the frequency with which the keys occur. The following intuitive code should work, but it fails! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-fe0d5202be91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhistogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrating\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhistogram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 3.0"
     ]
    }
   ],
   "source": [
    "histogram = {}\n",
    "for rating in [3.0, 5.0, 3.0, 2.0, 5.0, 1.5]:\n",
    "    histogram[rating] += 1\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code fails because python interprets `histogram[rating] += 1` as `histogram[rating] = histogram[rating] + 1`. Consider the very first iteration of the loop, when `rating` equals `3.0`. Python will try to calculate `histogram[3.0]`, and it will fail because histogram is empty!\n",
    "\n",
    "We could work around this error by placing conditionals around the increment that check to see if the key already exists, but this is ugly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.5: 1, 2.0: 1, 3.0: 2, 5.0: 2}\n"
     ]
    }
   ],
   "source": [
    "histogram = {}\n",
    "for rating in [3.0, 5.0, 3.0, 2.0, 5.0, 1.5]:\n",
    "    if rating not in histogram:\n",
    "        histogram[rating] = 0\n",
    "    histogram[rating] += 1\n",
    "print(histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, python provides us with a special variant of a dictionary that returns default values for keys that don't exist called defaultdict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "histogram = collections.defaultdict(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates an enhanced dictionary that is very useful for dictionaries that contain accumulator variables. When a key that doesn't exist is request, the dictionary returns the result of `int()` (which returns `0`). You can replace the function `int` with `float`, or even `list` or `set` depending on the type of accumulator value you want.\n",
    "\n",
    "Putting all the pieces together and normalizing the histogram to percentages, we get the following code to calculate the histogram for all 10M movielens ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('raw histogram of', 10000054, 'ratings is:')\n",
      "{0.5: 94988,\n",
      " 1.0: 384180,\n",
      " 1.5: 118278,\n",
      " 2.0: 790306,\n",
      " 2.5: 370178,\n",
      " 3.0: 2356676,\n",
      " 3.5: 879764,\n",
      " 4.0: 2875850,\n",
      " 4.5: 585022,\n",
      " 5.0: 1544812}\n",
      "()\n",
      "('percentages of rating values for', 10000054, 'ratings is:')\n",
      "{0.5: 0,\n",
      " 1.0: 3,\n",
      " 1.5: 1,\n",
      " 2.0: 7,\n",
      " 2.5: 3,\n",
      " 3.0: 23,\n",
      " 3.5: 8,\n",
      " 4.0: 28,\n",
      " 4.5: 5,\n",
      " 5.0: 15}\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import collections       # for defaultdict\n",
    "\n",
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat'  # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "# accumulator histogram of raw counts\n",
    "histogram = collections.defaultdict(int)\n",
    "for line in open(path):\n",
    "    tokens = line.split('::')    # tokens is a list of the four values in each record\n",
    "    rating = float(tokens[2])\n",
    "    histogram[rating] += 1\n",
    "        \n",
    "num_ratings = sum(histogram.values())\n",
    "\n",
    "# convert to percentages\n",
    "percents = {}\n",
    "for rating_value in histogram:\n",
    "    percents[rating_value] = 100 * histogram[rating_value] / num_ratings\n",
    "    \n",
    "# display results\n",
    "import pprint # for human-readable printing\n",
    "print('raw histogram of', num_ratings, 'ratings is:')\n",
    "pprint.pprint(dict(histogram))\n",
    "print()\n",
    "print('percentages of rating values for', num_ratings, 'ratings is:')\n",
    "pprint.pprint(dict(percents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the only data structures we kept in memory where the two dictionaries (histogram and percents). In total we needed less than a kilobyte of memory (0.1% of a MB) to process this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Reusing datafile parsing code\n",
    "\n",
    "This section of the lesson will show you how to write reusable functions that handle parsing input data into records. It should take you about 30 minutes to work through this section.\n",
    "\n",
    "So far we've been treating the ML10M dataset as a collection of individual records, one per line. This makes the code that parses a record into into its components remarkably simple. \n",
    "\n",
    "What if we wanted to shift to treating ML10M as a collection of users - not records. The natural data structure for each user is a dictionary with keys movie ids (ints) and values ratings (floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    user_ratings = { 456 : 0.5, 384 : 4.5, 121 : 3.0, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the file is sorted by user, so we could still stream a user into memory at a time to conserve memory. The pseudocode for this method is relatively simple:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    user_ratings = {}\n",
    "    for line in file\n",
    "        split line into user id, movie id, rating\n",
    "        if line contains a new user id:\n",
    "            process last user's ratings\n",
    "            user_ratings = {}\n",
    "        user_ratings[movie_id] = rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the actual python code looks more complex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "{256: 3.0, 1792: 2.0, 898: 4.0, 260: 5.0, 1982: 1.0, 1580: 3.0, 1986: 1.0, 780: 4.0, 1805: 4.0, 788: 4.0, 1690: 3.0, 32: 3.0, 1584: 3.0, 2338: 2.0, 1983: 1.0, 1320: 4.0, 1833: 3.0, 1196: 4.0, 1200: 5.0, 1717: 1.0, 1080: 4.0, 1721: 4.0, 1210: 4.0, 2107: 1.0, 316: 4.0, 829: 4.0, 1214: 5.0, 1909: 2.0, 1984: 1.0, 1985: 1.0, 1920: 4.0, 196: 4.0, 1356: 5.0, 589: 4.0, 2126: 2.0, 2384: 2.0, 1748: 3.0, 2012: 3.0, 442: 2.0, 480: 4.0, 1769: 3.0, 2028: 5.0, 110: 5.0, 1391: 4.0, 1136: 5.0, 1396: 3.0, 1598: 2.0, 2294: 5.0, 1527: 5.0, 1876: 3.0, 891: 1.0, 1917: 4.0, 1407: 2.0}\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat' # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "user_ratings = {}             # dictionary from movie id to user rating\n",
    "prev_user_id = -1     #  user id\n",
    "\n",
    "for line in open(path):\n",
    "    tokens = line.split('::')    # tokens is a list of the four values in each record\n",
    "    user_id = int(tokens[0])\n",
    "    movie_id = int(tokens[1])\n",
    "    rating = float(tokens[2])\n",
    "    \n",
    "    # if the current line represents a new user, process the previous user's ratings\n",
    "    if user_id != prev_user_id and len(user_ratings) > 0:\n",
    "        # YOUR ALGORITHMIC PROCESSING \"WORK\" FOR THE USER WOULD GO HERE\n",
    "        # SINCE THIS IS AN EXAMPLE, WE HAVE NOTHING\n",
    "        \n",
    "        # mark new user as current user, clear out old user's ratings\n",
    "        user_ratings.clear()\n",
    "        prev_user_id = user_id        \n",
    "    \n",
    "    # record user rating\n",
    "    user_ratings[movie_id] = rating\n",
    "    prev_id = user_id\n",
    "    \n",
    "print len(user_ratings)\n",
    "print user_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yuck!  If I want to analyze a stream of users somewhere else in my codebase, I have to duplicate all this code, breaking the [\"Don't Repeat Yourself\" (DRY) principle](http://en.wikipedia.org/wiki/Don't_repeat_yourself). To work around this, it's best to restructure your data parsing code to use a \"generator\" using Python's [\"yield\" keyword](http://stackoverflow.com/questions/231767/the-python-yield-keyword-explained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num_users:', 69879)\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def read_users(path):\n",
    "    user_ratings = {}     # dictionary from movie id to user rating\n",
    "    prev_user_id = -1     #  user id\n",
    "    \n",
    "    for line in open(path):\n",
    "        tokens = line.split('::')    # tokens is a list of the four values in each record\n",
    "        user_id = int(tokens[0])\n",
    "        movie_id = int(tokens[1])\n",
    "        rating = float(tokens[2])\n",
    "            \n",
    "        # if the current line represents a new user, process the previous user's ratings\n",
    "        if user_id != prev_user_id and len(user_ratings) > 0:\n",
    "            # this tells Python to use the value of user_ratings \n",
    "            # for the next iteration of the for loop at the bottom of this cell\n",
    "            yield user_ratings\n",
    "            \n",
    "            # mark new user as current user, clear out old user's ratings\n",
    "            user_ratings.clear()\n",
    "            prev_user_id = user_id        \n",
    "        \n",
    "        # record user rating\n",
    "        user_ratings[movie_id] = rating\n",
    "        prev_id = user_id\n",
    "    \n",
    "    # process the very last user in the file\n",
    "    yield user_ratings\n",
    "\n",
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat'  # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "num_users = 0\n",
    "for user in read_users(path):\n",
    "    num_users += 1\n",
    "print('num_users:', num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Managing noise in large datasets\n",
    "\n",
    "<p>Social data typically follows the [90 / 9 / 1 rule](http://www.nngroup.com/articles/participation-inequality/). Only 10% of users contribute data, and 1% of users contribute most of the data. These types of inequality appear across many entities. For example, in Wikipedia, this same inequality appears when you study edits per user, edits per article, and articles per language.</p>\n",
    "\n",
    "<b>Summary: In most online communities, 90% of users are lurkers who never contribute, 9% of users contribute a little, and 1% of users account for almost all the action.</b>\n",
    "\n",
    "<p>To effectively manage these lopsided datasets, your analyses will need to cope with wide differences in sample sizes within social data. Take our movie rating examples. Let's say we are trying to rank movies by mean rating. Should a movie with 2 ratings of 5.0 be ranked higher than a movie with 1000 ratings averaging 4.9? Your intitution probably says no.</p>\n",
    "\n",
    "<p>\n",
    "A common technique to account for varying sample sizes across groups (i.e. ratings grouped by movie) adds a fixed number of values following the overall expected distribution to each group. In this case, we would add a fixed number of ratings with the overall mean rating of 3.51:\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust_mean 3.1750 for 1 ratings with real mean 1.5000\n",
      "robust_mean 3.9357 for 2 ratings with real mean 5.0000\n",
      "robust_mean 4.9005 for 100 ratings with real mean 4.9700\n",
      "robust_mean 4.8931 for 1000 ratings with real mean 4.9000\n"
     ]
    }
   ],
   "source": [
    "num_fake_ratings = 5\n",
    "mean_rating = 3.51\n",
    "\n",
    "def robust_mean(rating_values):\n",
    "    num_ratings = len(rating_values) + num_fake_ratings\n",
    "    sum_ratings = sum(rating_values) + num_fake_ratings * mean_rating\n",
    "    return sum_ratings / num_ratings\n",
    "\n",
    "example_movies = [\n",
    "    [1.5] * 1,          # one rating of 1.5\n",
    "    [5.0] * 2,          # two ratings of 5.0\n",
    "    [4.97] * 100,       # 100 ratings averaging 4.95\n",
    "    [4.9] * 1000,       # 1000 ratings averaging 4.9\n",
    "]\n",
    "\n",
    "for rating_values in example_movies:\n",
    "    n = len(rating_values)\n",
    "    real_mean = 1.0 * sum(rating_values) / len(rating_values)\n",
    "    print('robust_mean %.4f for %d ratings with real mean %.4f' \n",
    "          %  (robust_mean(rating_values), n, real_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we arbitrarily set the number of fake ratings to 5. If we bump the number of fake ratings up to 10, our robust mean estimate becomes more \"conservative:\"\n",
    "\n",
    "<pre>\n",
    "    robust_mean 3.3273 for 1 ratings with real mean 1.5000\n",
    "    robust_mean 3.7583 for 2 ratings with real mean 5.0000\n",
    "    robust_mean 4.8373 for 100 ratings with real mean 4.9700\n",
    "    robust_mean 4.8862 for 1000 ratings with real mean 4.9000\n",
    "</pre>\n",
    "\n",
    "This technique can also be applied to binary data, such as thumbs up / thumbs down ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust_porportion 0.3333 for 1 ratings with real proportion 0.0000\n",
      "robust_porportion 0.5714 for 2 ratings with real proportion 1.0000\n",
      "robust_porportion 0.9429 for 100 ratings with real proportion 0.9700\n",
      "robust_porportion 0.9174 for 1000 ratings with real proportion 0.9200\n"
     ]
    }
   ],
   "source": [
    "# Throughout this example a value of 0 represents thumbs down and 1 represents thumbs up.\n",
    "num_fake_ratings = 5\n",
    "mean_fraction_up = 0.4    # 40% thumbs up\n",
    "\n",
    "def robust_proportion(thumb_values):\n",
    "    num_ratings = len(thumb_values) + num_fake_ratings\n",
    "    sum_ratings = sum(thumb_values) + num_fake_ratings * mean_fraction_up\n",
    "    return sum_ratings / num_ratings\n",
    "\n",
    "# Thumb ratings for a collection of exaple movies.\n",
    "example_movies = [\n",
    "    [0] * 1,                    # one thumbs down\n",
    "    [1.0] * 2,                  # two thumbs up\n",
    "    [1.0] * 97 + [0.0] * 3,     # 97 thumbs up, 3 thumbs down\n",
    "    [1.0] * 920 + [0.0] * 80    # 920 thumbs up, 80 thumbs down\n",
    "]\n",
    "\n",
    "for thumb_values in example_movies:\n",
    "    n = len(thumb_values)\n",
    "    real_prop = 1.0 * sum(thumb_values) / len(thumb_values)\n",
    "    print('robust_porportion %.4f for %d ratings with real proportion %.4f' \n",
    "          %  (robust_proportion(thumb_values), n, real_prop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique has a deep justification based on Bayesian conjugate priors. Take one of statistics.com's Bayesian course to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#5. Introduction to recommendation algorithms\n",
    "\n",
    "This section of the lesson describes one key recommendation algorithm used in sites such as Netflix and Amazon to identify related products. At the end of this lesson, you'll understand how to identify related items based on rating patterns, and you'll be more skilled at stream-based data analysis. It should take you about 30 minutes to work through this section.\n",
    "\n",
    "Imagine you've been tasked with building an association based recommender for MovieLens. These recommenders look for strongly related items (i.e. movies) to create rules such as \"If you like *Finding Nemo*, you'll like *Toy Story*.\"\n",
    "\n",
    "Given a *target* item (e.g. *Finding Nemo*), association-based recommenders typically calculate the similarity between the target and every other *candidate* item and return the most similar items. [Pearson's Correlation](http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) is most commonly used as the measure of similarity between items. \n",
    "\n",
    "More specifically, let $X = \\{x_1,...,x_n\\}$ contain ratings for the target item (e.g. *Finding Nemo*) and $Y = \\{y_1,...,y_n\\}$ contain the ratings for a candidate item (e.g. *Toy Story*). Each vector only contains ratings from users who rated *both* movies, and the ratings are paired, so $x_1$ and $y_1$ are user 1's ratings for both movies, etc. Pearson's correlation $r_p$ is:\n",
    "\n",
    "$$\n",
    "r_p(X, Y)  =\n",
    "\\frac{cov(X, Y)}{\\sigma_X \\sigma_Y} = \n",
    "\\frac{\\sum ^n _{i=1}(x_i - \\bar{X})(y_i - \\bar{Y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{X})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{Y})^2}}\n",
    "$$\n",
    "\n",
    "To calculate this correlation, we can use scipy's pearson r, which returns both $p_r$ and the $p$ value for $r_s$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.981980506062\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "(r_s, p) = pearsonr([3, 5, 6], [3, 2, 1])\n",
    "print(r_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll lookup some movie ids to calculate the correlation between. You can find the mapping from movie title to movie id by opening the `movie.dat` in the ML10M zip file using NotePad, TextEdit, etc. Here are a few interesting possibilities:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    1::Toy Story (1995)::Adventure|Animation|Children|Comedy|Fantasy\n",
    "    ...\n",
    "    596::Pinocchio (1940)::Animation|Children|Fantasy|Musical\n",
    "    ...\n",
    "    4474::Beaches (1988)::Comedy|Drama|Musical\n",
    "    ...\n",
    "    6377::Finding Nemo (2003)::Adventure|Animation|Children|Comedy\n",
    "    ...\n",
    "    6874::Kill Bill: Vol. 1 (2003)::Action|Crime|Thriller\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to calculate the correlation between two movies, all we need to do is find each user who has rated both movies, add append their ratings for each movie to the X and Y vector. Given our earlier `readUsers` function, we're in good shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0c0dcbdb3259>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'\\ndef movie_similarity(path, movie_id1, movie_id2):\\n    \"\"\"\\n        Takes the path to the movie.dat and two movie ids (as ints).\\n        Returns the pearson correlation between the movie ids.\\n    \"\"\"\\n    X = []\\n    Y = []\\n    \\n    for user in readUsers(path):\\n        # If the user has rated both movies, add their ratings to X and Y\\n        if movie_id1 in user and movie_id2 in user:\\n            X.append(user[movie_id1])\\n            Y.append(user[movie_id2])\\n            \\n    r_s, p = pearsonr(X, Y)\\n    return r_s\\n\\nmovies = {\\n          1 : \\'Toy Story\\',\\n          596 : \\'Pinnochio\\',\\n          4474 : \\'Beaches\\',\\n          6377 : \\'Finding Nemo\\',\\n          6874 : \\'Kill Bill: Vol. 1\\',\\n    }\\n\\nfor movie_id in movies:\\n    title = movies[movie_id]\\n    sim = movie_similarity(path, 1, movie_id)\\n    print(\\'similarity between Toy Story and\\', title, \\'is\\', sim)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2259\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2260\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2261\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2262\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def movie_similarity(path, movie_id1, movie_id2):\n",
    "    \"\"\"\n",
    "        Takes the path to the movie.dat and two movie ids (as ints).\n",
    "        Returns the pearson correlation between the movie ids.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for user in readUsers(path):\n",
    "        # If the user has rated both movies, add their ratings to X and Y\n",
    "        if movie_id1 in user and movie_id2 in user:\n",
    "            X.append(user[movie_id1])\n",
    "            Y.append(user[movie_id2])\n",
    "            \n",
    "    r_s, p = pearsonr(X, Y)\n",
    "    return r_s\n",
    "\n",
    "movies = {\n",
    "          1 : 'Toy Story',\n",
    "          596 : 'Pinnochio',\n",
    "          4474 : 'Beaches',\n",
    "          6377 : 'Finding Nemo',\n",
    "          6874 : 'Kill Bill: Vol. 1',\n",
    "    }\n",
    "\n",
    "for movie_id in movies:\n",
    "    title = movies[movie_id]\n",
    "    sim = movie_similarity(path, 1, movie_id)\n",
    "    print('similarity between Toy Story and', title, 'is', sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that this is a slow calculation! For your homework assignment, you'll simultaneously calculate the correlation between Toy Story and the other movies in a single pass to speed things up.\n",
    "\n",
    "The [item-based recommender algorithm](http://stackoverflow.com/a/16447806/141245), a popular algorithms used by companies such as Netflix and Amazon, is closely based on Pearson correlation. The algorithm simply predicts that your rating for a movie (i.e. Toy Story) is the weighted average of your ratings for movies that are similar to it. The weights are the correlations calculated above.  \n",
    "\n",
    "For example presume you rated Kill Bill a 1.5, Pinnochio 4.0, and Finding Nemo 5.0. Our python code above estimated the correlation for these movies  0.11, 0.36, and 0.49 respectively. The item-based algorithm would predict your rating for Toy Story is:\n",
    "\n",
    "$$ prediction(Toy Story) = \\frac{0.11 * 1.5 + 0.36 * 4.0 + 0.49 * 5.0}{0.11 + 0.36 + 0.49} = 4.22 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Algorithmic time complexity in a nutshell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part of this lesson, you'll start learning to characterize the time complexity of your code. It will take you about an hour to work through this section.\n",
    "\n",
    "When we analyze time complexity, we talk about how an algorithm \"scales up.\" As we add more movies, users, or ratings, how much slower will the algorithm be? For example, consider the Python program above to calculate the overall average rating in the MovieLens 10M dataset. How does it change if we increase the size of the dataset? Since the main loop iterates over each rating, it makes sense that if we increase the size of the dataset by a factor of two (to 20M ratings), the program will be twice as slow.\n",
    "\n",
    "We call this type of algorithm performance \"linear\" because it scales linearly with the size of the dataset. More formally, many people refer to this type of time complexity `O(n)` (pronounced \"Big-Oh of n\"), where `n` is the number of ratings of the dataset.\n",
    "\n",
    "Other than linear, several other algorithmic time complexities appear frequently. Getting the length of a list or dictionary using `len()` always takes roughly the same amount of time - regardless of how many things are in the list! This time complexity is called \"constant\" or `O(1)`.\n",
    "\n",
    "Take a few moments to watch MyCodeSchool's YouTube video on calculating algorithmic time complexity:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/8syQKTdgdzc\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x40ca3c8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# a talk about IPython at Sage Days at U. Washington, Seattle.\n",
    "# Video credit: William Stein.\n",
    "YouTubeVideo('8syQKTdgdzc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the time complexity of iterating over n ratings and counting the number of high ratings:\n",
    "\n",
    "    ratings = [3.0, 4.0, .... ]\n",
    "    num_high = 0\n",
    "    for r in ratings:\n",
    "        if r > 3.5:\n",
    "            num_high += 1\n",
    "\n",
    "This algorithm has linear time complexity. The innermost loop runs once for each rating, and the performance of the code in a single iteration(evaluating the \"if\" statement and incrementing num_high) does not depend on the number of ratings.\n",
    "\n",
    "On the other hand, consider the code to find the nearest neighbor for each user:\n",
    "\n",
    "    pairs = {}\n",
    "    for u1 in users:\n",
    "    \n",
    "        # find the closest user for u1\n",
    "        closest_u = 0\n",
    "        closest_similarity = 0\n",
    "        for u2 in users:\n",
    "            s = similarity(u1, u2)\n",
    "            if s > closest_similarity:\n",
    "                closest_similarity = s\n",
    "                closest_u = u2\n",
    "                \n",
    "        # record the \n",
    "        pairs[u1] = closest_u\n",
    "\n",
    "This code has *nested* loops. Let's call the number of users n. For each of the n iterations of the outer loop, the inner loop also runs n times. Thus, the innermost loop has time a complexity of $O(n^2)$ However, this doesn't consider any additional time complexity related to the call to `similarity(u1, u2)`. If this function does not take a constant amount of time, the time complexity will change.\n",
    "\n",
    "**Time complexity of basic data structure operations:** In general, for all the data structures from section 1 (list, dict, set, tuple) the basic operations  (add, delete, retrieve, membership using \"in\") are constant time and do not increase with the size of the collection. One major exception is list deleting and list membership checking, which are both linear. The Python wiki has a more detailed analysis of [Python data structure time complexity](https://wiki.python.org/moin/TimeComplexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
