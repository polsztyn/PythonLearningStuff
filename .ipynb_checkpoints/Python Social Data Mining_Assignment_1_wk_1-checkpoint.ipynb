{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Social data mining, Shilad Sen\n",
    "#Assignment 1: 35 points\n",
    "\n",
    "##Question 1: Modeling tweets\n",
    "\n",
    "<style type=\"text/css\">\n",
    "iframe.twitter-tweet {\n",
    "width:800px !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "**Background:** In this question you will practice creating and navigating data structures that represent real world social media. At the end of this question you should be more comfortable translating social media into Python data structures. \n",
    "\n",
    "**Completion time:** This question should take you approximately one hour. \n",
    "\n",
    "**Point value:** Ungraded, immediate feedback.\n",
    "\n",
    "**Part A.** Write Python code that declares a variable called `tweets` and fills it with three tweets.\n",
    "\n",
    " * The top level `tweets` data structure should be organized by username. Given a username, we should be able to easily retrieve the user's tweets.\n",
    " * Each tweet should contain three attributes: an author's username, the text of the tweet, and the usernames that have favorited the tweet. **You only need to include a few usernames that favorite each tweet.**\n",
    " * You may find it easier to fill the tweets variable in several steps, instead of defining it all at once.\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" lang=\"en\" width=\"100%\"><p>News! Based on <a href=\"https://twitter.com/NateSilver538\">@NateSilver538</a>&#39;s calculations, there&#39;s a 90.617854% chance we’re relaunching FiveThirtyEight on March 17.</p>&mdash; FiveThirtyEight (@FiveThirtyEight) <a href=\"https://twitter.com/FiveThirtyEight/statuses/442372247379247105\">March 8, 2014</a></blockquote>\n",
    "<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" lang=\"en\"><p>The Fox knows many little things; The hedgehog knows one big thing. <a href=\"http://t.co/GWSmHaatPD\">http://t.co/GWSmHaatPD</a> <a href=\"http://t.co/4s6rNpp5zi\">http://t.co/4s6rNpp5zi</a></p>&mdash; FiveThirtyEight (@FiveThirtyEight) <a href=\"https://twitter.com/FiveThirtyEight/statuses/442384984540999680\">March 8, 2014</a></blockquote>\n",
    "<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" lang=\"en\"><p>What scientific idea is ready for retirement? It is the standard deviation! (according to Nassim Taleb) <a href=\"http://t.co/0xvKRa1UNb\">http://t.co/0xvKRa1UNb</a></p>&mdash; Jure Leskovec (@jure) <a href=\"https://twitter.com/jure/statuses/423611752417202176\">January 16, 2014</a></blockquote>\n",
    "<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "\n",
    "**Part B.** Write Python code to count and print out the total number of times each users' tweets have been favorited. As of March 9, 2014, this would be:\n",
    "\n",
    "<pre>\n",
    "    ('number of favorites for tweeter', 'FiveThirtyEight', 273)\n",
    "    ('number of favorites for tweeter', 'Jure Leskovec', 13)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "('number of favorties for tweeter', 'jure', 3)\n",
      "('number of favorties for tweeter', 'paul', 0)\n",
      "('number of favorties for tweeter', 'FiveThirtyEight', 5)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "user_tweets = collections.defaultdict(list) # this is a dictionary of lists.  Should return an empty list if we ash for a non-indexed value\n",
    "print user_tweets['paul'] # should return an emptylist\n",
    "\n",
    "# each item in our dictionay is a list of dictionaries.  the favorited dictionary item is a set.\n",
    "user_tweets['FiveThirtyEight'].append({\n",
    "           'userName' : 'FiveThirtyEight',\n",
    "           'text' : 'News! Based on @NateSilver538\\'s calculations, there\\'s a 90.617854% chance we’re relaunching FiveThirtyEight on March 17',\n",
    "           'favorited' : set(['TomSeleck', 'GraceJones','TommyTall'])\n",
    "    })\n",
    "user_tweets['FiveThirtyEight'].append({\n",
    "           'userName' : 'FiveThirtyEight',\n",
    "           'text' : 'The Fox knows many little things; The hedgehog knows one big thing.  http://www.fivethirtyeight.com/2014/03/fivethirtyeight-to-relaunch-on-march-17.html http://bit.ly/1ilaNwl',\n",
    "           'favorited' : set(['BigBen', 'ThomasEnsley'])\n",
    "    })\n",
    "\n",
    "user_tweets['jure'].append({\n",
    "           'userName' : 'jure',\n",
    "           'text' : 'What scientific idea is ready for retirement? It is the standard deviation! (according to Nassim Taleb) http://www.edge.org/response-detail/25401 …',\n",
    "           'favorited' : set(['Kit Carson', 'Davy_Jones','Michael the Archangel'])\n",
    "    })\n",
    "\n",
    "        \n",
    "# loop over each user\n",
    "for user in user_tweets:\n",
    "    \n",
    "    # accumulate sum over all tweets by that user    \n",
    "    # alternately, with fancy list comprehensions: \n",
    "    # n = sum([len(tweet['favorited']) for tweet in user_tweets[user])\n",
    "    n = 0\n",
    "    for tweet in user_tweets[user]:\n",
    "        n += len(tweet['favorited'])\n",
    "        \n",
    "    print('number of favorties for tweeter', user, n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Finding the highest rated movies.\n",
    "\n",
    "**Background:** In this question you will practice dealing with noisy social data that has widely varying sample sizes. Upon completing this assignment, you will 1) Know how to estimate simple values like the mean in a way that accounts for small sample sizes. 2) Write programs that analyze data in a streaming structure. 3) Use dictionaries (and more specifically Python's defaultdict) to create an accumulator for different groups of samples. \n",
    "\n",
    "**Completion time:** This question should take you about two hours.\n",
    "\n",
    "**Point value:** 10 points\n",
    "\n",
    "**Instructions:** Write  a Python program that calculates the robust means for all movies in the MovieLens 10M dataset. Note: your program **must not** store all the ratings for each movie in memory. \n",
    "\n",
    " 1. Your program should create an accumulator variable called `sums` that keeps track of the sum of ratings for each movies.\n",
    " 1. Your program should create an accumulator variable called `counts` that keeps track of the count of ratings for each movie.\n",
    " 1. Your program should stream through each movie rating.\n",
    " 1. For each rating, your program should update the two accumulator data structures.\n",
    " 1. Your program should create a dictionary called `robust_means` whose keys will be movie ids and values will be robust means.\n",
    " 1. Your program should fill the `robust_means` dictionary using data in the `sums` and `counts` accumulators.\n",
    " 1. Your program should print the movie id, number of ratings, real mean, and robust mean for the 20 movies with highest and lowest robust means. Hint: You'll need to sort the robust_means dictionary by value. See [this Stackoverflow post](http://stackoverflow.com/questions/613183/python-sort-a-dictionary-by-value/3177911#3177911) for help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Movies by Sorted by Robust Mean\n",
      "movie_id = 318 Shawshank Redemption, The (1994) num movie ratings = 31126.0 robust_mean = 4.45708657318 real mean = 4.45723832166\n",
      "movie_id = 858 Godfather, The (1972) num movie ratings = 19814.0 robust_mean = 4.41485756646 real mean = 4.41508529323\n",
      "movie_id = 50 Usual Suspects, The (1995) num movie ratings = 24037.0 robust_mean = 4.36696456658 real mean = 4.36714232225\n",
      "movie_id = 527 Schindler's List (1993) num movie ratings = 25777.0 robust_mean = 4.36331790046 real mean = 4.36348294992\n",
      "movie_id = 922 Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) num movie ratings = 3255.0 robust_mean = 4.32072457352 real mean = 4.32196620584\n",
      "movie_id = 912 Casablanca (1942) num movie ratings = 12507.0 robust_mean = 4.31941832718 real mean = 4.31974094507\n",
      "movie_id = 904 Rear Window (1954) num movie ratings = 8825.0 robust_mean = 4.31608857414 real mean = 4.31654390935\n",
      "movie_id = 3435 Double Indemnity (1944) num movie ratings = 2403.0 robust_mean = 4.31377164023 real mean = 4.31543903454\n",
      "movie_id = 2019 Seven Samurai (Shichinin no samurai) (1954) num movie ratings = 5751.0 robust_mean = 4.31342288215 real mean = 4.3141192836\n",
      "movie_id = 1212 Third Man, The (1949) num movie ratings = 3265.0 robust_mean = 4.31240431488 real mean = 4.31362940276\n",
      "movie_id = 1178 Paths of Glory (1957) num movie ratings = 1778.0 robust_mean = 4.30457773958 real mean = 4.30680539933\n",
      "movie_id = 1221 Godfather: Part II, The (1974) num movie ratings = 13281.0 robust_mean = 4.3029175154 real mean = 4.30321511934\n",
      "movie_id = 750 Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) num movie ratings = 11774.0 robust_mean = 4.29773852701 real mean = 4.2980720231\n",
      "movie_id = 44555 Lives of Others, The (Das Leben der Anderen) (2006) num movie ratings = 1230.0 robust_mean = 4.29397741673 real mean = 4.29715447154\n",
      "movie_id = 58559 Dark Knight, The (2008) num movie ratings = 2598.0 robust_mean = 4.2933392661 real mean = 4.2948421863\n",
      "movie_id = 1193 One Flew Over the Cuckoo's Nest (1975) num movie ratings = 14435.0 robust_mean = 4.29210956438 real mean = 4.29237963284\n",
      "movie_id = 1148 Wallace & Gromit: The Wrong Trousers (1993) num movie ratings = 7932.0 robust_mean = 4.27542674936 real mean = 4.27590771558\n",
      "movie_id = 3030 Yojimbo (1961) num movie ratings = 1693.0 robust_mean = 4.27536048861 real mean = 4.27761370348\n",
      "movie_id = 745 Wallace & Gromit: A Close Shave (1995) num movie ratings = 6332.0 robust_mean = 4.27435097202 real mean = 4.2749526216\n",
      "movie_id = 4454 More (1998) num movie ratings = 8.0 robust_mean = 4.27400843574 real mean = 4.75\n",
      "\n",
      "\n",
      "Bottom 20 Movies by Sorted by Robust Mean\n",
      "movie_id = 6483 From Justin to Kelly (2003) num movie ratings = 216.0 robust_mean = 0.982181491695 real mean = 0.923611111111\n",
      "movie_id = 8859 SuperBabies: Baby Geniuses 2 (2004) num movie ratings = 61.0 robust_mean = 1.0085168131 real mean = 0.803278688525\n",
      "movie_id = 6371 Pokémon Heroes (2003) num movie ratings = 156.0 robust_mean = 1.08734229605 real mean = 1.00961538462\n",
      "movie_id = 4775 Glitter (2001) num movie ratings = 380.0 robust_mean = 1.19756392121 real mean = 1.16710526316\n",
      "movie_id = 61348 Disaster Movie (2008) num movie ratings = 40.0 robust_mean = 1.20138021477 real mean = 0.9125\n",
      "movie_id = 6587 Gigli (2003) num movie ratings = 352.0 robust_mean = 1.20745688982 real mean = 1.17471590909\n",
      "movie_id = 1826 Barney's Great Adventure (1998) num movie ratings = 234.0 robust_mean = 1.22410924546 real mean = 1.17521367521\n",
      "movie_id = 5672 Pokemon 4 Ever (a.k.a. Pokémon 4: The Movie) (2002) num movie ratings = 233.0 robust_mean = 1.22505088094 real mean = 1.17596566524\n",
      "movie_id = 3574 Carnosaur 3: Primal Species (1996) num movie ratings = 79.0 robust_mean = 1.26264416267 real mean = 1.12025316456\n",
      "movie_id = 31698 Son of the Mask (2005) num movie ratings = 178.0 robust_mean = 1.34460169216 real mean = 1.28370786517\n",
      "movie_id = 5739 Faces of Death 6 (1996) num movie ratings = 94.0 robust_mean = 1.35416272388 real mean = 1.23936170213\n",
      "movie_id = 6872 House of the Dead, The (2003) num movie ratings = 231.0 robust_mean = 1.36890724434 real mean = 1.32251082251\n",
      "movie_id = 1495 Turbo: A Power Rangers Movie (1997) num movie ratings = 440.0 robust_mean = 1.3844092352 real mean = 1.36022727273\n",
      "movie_id = 8811 Yu-Gi-Oh! (2004) num movie ratings = 92.0 robust_mean = 1.40785680067 real mean = 1.29347826087\n",
      "movie_id = 3027 Slaughterhouse 2 (1988) num movie ratings = 36.0 robust_mean = 1.42834413816 real mean = 1.13888888889\n",
      "movie_id = 5738 Faces of Death 5 (1996) num movie ratings = 85.0 robust_mean = 1.44513455183 real mean = 1.32352941176\n",
      "movie_id = 4241 Pokémon 3: The Movie (2001) num movie ratings = 263.0 robust_mean = 1.45358996144 real mean = 1.4144486692\n",
      "movie_id = 5740 Faces of Death: Fact or Fiction? (1999) num movie ratings = 64.0 robust_mean = 1.4574218792 real mean = 1.296875\n",
      "movie_id = 1990 Prom Night IV: Deliver Us From Evil (1992) num movie ratings = 101.0 robust_mean = 1.45813311004 real mean = 1.35643564356\n",
      "movie_id = 5647 Ernest Goes to Africa (1997) num movie ratings = 134.0 robust_mean = 1.4644756091 real mean = 1.38805970149\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat'  # replace this with the full path to the ratings file on your computer\n",
    "movieTitlePath = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/movies.dat'  # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "import collections\n",
    "\n",
    "# accumulator histogram of raw counts\n",
    "histogram = collections.defaultdict(int)\n",
    "\n",
    "# accumulators necessary for computing mean vaues of each movie.\n",
    "sums = collections.defaultdict(int)    #keeps track of the sum of ratings for each movies\n",
    "counts = collections.defaultdict(float)  #keeps track of the count of ratings for each movie\n",
    "robustMeans = {} # dictionray to hold computed robust means for each, key = movieID\n",
    "realMeans = {} # dictionray to hold computed robust means for each, key = movieID\n",
    "numFakeRatings=5\n",
    "\n",
    "# read in movie titles\n",
    "movieTitles = collections.defaultdict(str)\n",
    "for line in open('C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/movies.dat'):\n",
    "    tokens = line.split('::')    # tokens is a list of the four values in each recor\n",
    "    movieID = int(tokens[0])\n",
    "    movieTitles[movieID] = tokens[1]\n",
    "\n",
    "totalMovieRatingSum=0.0\n",
    "totalMovieRatingCount=0\n",
    "\n",
    "fp = open(path,\"r\")\n",
    "for line in fp:\n",
    "    tokens = line.split('::')    # tokens is a list of the four values in each recor\n",
    "    \n",
    "    # accumulate individual movie ratings and counts\n",
    "    movieID = int(tokens[1])\n",
    "    movieRatingValue =  float(tokens[2])\n",
    "    \n",
    "    counts[movieID] +=1\n",
    "    sums[movieID] += movieRatingValue\n",
    "    \n",
    "    # accumulate total movie ratings and counts to compute the grand average for all movies\n",
    "    # to use a the value for our fake ratings.\n",
    "    totalMovieRatingSum+=movieRatingValue\n",
    "    totalMovieRatingCount+=1\n",
    "    \n",
    "\n",
    "# compute the value for our fake rating padding\n",
    "fakeRating = totalMovieRatingSum/totalMovieRatingCount\n",
    "\n",
    "#compute robustMeans and actual for all movies\n",
    "for movieID in counts.keys():\n",
    "    realMeans[movieID]= sums[movieID]/counts[movieID]\n",
    "    robustMeans[movieID]= (sums[movieID]+(numFakeRatings*fakeRating))/(counts[movieID]+numFakeRatings)\n",
    "\n",
    "  \n",
    "fp.close()\n",
    "\n",
    "print \"Top 20 Movies by Sorted by Robust Mean\"\n",
    "for w in (sorted(robustMeans, key=robustMeans.get, reverse=True))[0:20]:\n",
    "  print \"movie_id = \" + str(w) + \" \" + movieTitles[w], \"num movie ratings = \" + str(counts[w]),  \"robust_mean = \" + str(robustMeans[w]), \"real mean = \" + str(realMeans[w]) \n",
    "print \"\\n\"\n",
    "\n",
    "print \"Bottom 20 Movies by Sorted by Robust Mean\"\n",
    "for w in (sorted(robustMeans, key=robustMeans.get, reverse=False))[0:20]:\n",
    "  print \"movie_id = \" + str(w) + \" \" + movieTitles[w], \"num movie ratings = \" + str(counts[w]),  \"robust_mean = \" + str(robustMeans[w]), \"real mean = \" + str(realMeans[w]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: If you liked \"Star Wars,\" you'll like ?\n",
    "\n",
    "**Introduction:** For this question, you will generalize the code above to simultaneously calculate the correlation with Star Wars and every other movie. Upon completion of this homework, 1) You'll know how to build an associative recommender system, and 2) You'll have gained experience in using nested accumulator data structures for streaming datasets.\n",
    "\n",
    "**Completion time:** This question should require 3-5 hours.\n",
    "\n",
    "**Point value:** 15 points\n",
    "\n",
    "**Generalizing the correlation algorithm:**\n",
    "Recall the structure of our cosimilarity code above. \n",
    "To calculate the correlation between Star Wars and Finding Nemo, we needed two lists: X and Y.\n",
    "Each list contained one entry for every user who rated both movies.\n",
    "For example, X[2] contains the rating for Toy Story for the third user who rated both movies, and Y[2] contains the same user's rating for Finding Nemo.\n",
    "\n",
    "To generalize this program to simultaneously calculate the correlation between ratings for Star Wars and every other movie, we need to simultaneously calculate the X and Y lists for every other movie. We'll use one data structure that calculates every movie's X called `every_X` and a similar datastructure called `every_Y`. These will be nested datastructures? What will they look like?\n",
    "\n",
    "Note that although `every_X` always contains user ratings for Star Wars, it will contain a different list of ratings for each candidate movie (i.e. the lists stored by `every_X` will be different for Lion King and Finding Nemo).\n",
    "This is because each list in `every_X` only includes ratings for users who rated both Star Wars and the candidate movie.\n",
    "\n",
    "**Helper function for reading movie titles:** Your program will need to look up a movie's title given its id. The read_titles method below returns a dictionary whose keys are movie ids (ints) and values are titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/shilad/Downloads/ml-10M100K/movies.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0d8f103d4ce0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpath_movies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/Users/shilad/Downloads/ml-10M100K/movies.dat'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_titles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_movies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-0d8f103d4ce0>\u001b[0m in \u001b[0;36mread_titles\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m      7\u001b[0m     \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'::'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/shilad/Downloads/ml-10M100K/movies.dat'"
     ]
    }
   ],
   "source": [
    "def read_titles(path):\n",
    "    \"\"\"\n",
    "        Read in the movie ids and their associated titles.\n",
    "        Returns a hashtable containing the association.\n",
    "        Note that the ids are ints.  \n",
    "    \"\"\"\n",
    "    titles = {}\n",
    "    for line in open(path, 'r'):\n",
    "        tokens = line.split('::')\n",
    "        if len(tokens) >= 2:\n",
    "            (id, title) = tokens[:2]\n",
    "            titles[int(id)] = title\n",
    "    return titles\n",
    "\n",
    "# replace this with the full path to the movies.dat on your computer\n",
    "path_movies = '/Users/shilad/Downloads/ml-10M100K/movies.dat' \n",
    "\n",
    "titles = read_titles(path_movies)\n",
    "for i in range(1, 10):\n",
    "    print(i, titles[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    " 1. Create two data structures to hold rating lists for every movie: `every_X` and `every_Y`. \n",
    "    Carefully plan the contents of these data structures and describe them in your source code.\n",
    " 1. While reading in users via the `read_users()` method, skip all users who have not rated Star Wars (movie id 260).\n",
    " 1. For each user who has rated Star Wars, update `every_X` and `every_Y` for each movie they have rated.\n",
    " 1. After you have finished processing all users, create a dictionary called `every_correlation`.\n",
    " 1. Fill `every_correlation` with the correlations for every movie with at least 1000 ratings using the data in `every_X` and `every_Y`.\n",
    " 1. Print the 20 movies along with highest correlations. You'll need to sort the dictionary by values, as you did in Assignment 1.2.  The first two movies you should see should be obvious choices for the movies most related to Star Wars.  If they aren't, you've done something wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1659\n",
      "Top 20 Movies You Might Like if You Like Star Wars\n",
      "movie_id = 260 Star Wars: Episode IV - A New Hope (a.k.a. Star Wars) (1977)  pearson correlation = 1.0 number of reviews = 28566\n",
      "movie_id = 1196 Star Wars: Episode V - The Empire Strikes Back (1980)  pearson correlation = 0.721649903836 number of reviews = 19572\n",
      "movie_id = 1210 Star Wars: Episode VI - Return of the Jedi (1983)  pearson correlation = 0.663125010211 number of reviews = 20649\n",
      "movie_id = 1198 Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)  pearson correlation = 0.462407332137 number of reviews = 17006\n",
      "movie_id = 33493 Star Wars: Episode III - Revenge of the Sith (2005)  pearson correlation = 0.414449461842 number of reviews = 4643\n",
      "movie_id = 2628 Star Wars: Episode I - The Phantom Menace (1999)  pearson correlation = 0.402800058672 number of reviews = 12491\n",
      "movie_id = 5378 Star Wars: Episode II - Attack of the Clones (2002)  pearson correlation = 0.394955311688 number of reviews = 6599\n",
      "movie_id = 5952 Lord of the Rings: The Two Towers, The (2002)  pearson correlation = 0.35655859915 number of reviews = 9439\n",
      "movie_id = 4993 Lord of the Rings: The Fellowship of the Ring, The (2001)  pearson correlation = 0.344250239611 number of reviews = 10661\n",
      "movie_id = 7153 Lord of the Rings: The Return of the King, The (2003)  pearson correlation = 0.342370207785 number of reviews = 8365\n",
      "movie_id = 1291 Indiana Jones and the Last Crusade (1989)  pearson correlation = 0.331218022753 number of reviews = 13330\n",
      "movie_id = 1240 Terminator, The (1984)  pearson correlation = 0.321103776769 number of reviews = 14115\n",
      "movie_id = 2640 Superman (1978)  pearson correlation = 0.319958433687 number of reviews = 7339\n",
      "movie_id = 2716 Ghostbusters (a.k.a. Ghost Busters) (1984)  pearson correlation = 0.316131168026 number of reviews = 10941\n",
      "movie_id = 1356 Star Trek: First Contact (1996)  pearson correlation = 0.310175830781 number of reviews = 9258\n",
      "movie_id = 480 Jurassic Park (1993)  pearson correlation = 0.305429598461 number of reviews = 16949\n",
      "movie_id = 1374 Star Trek II: The Wrath of Khan (1982)  pearson correlation = 0.305375354947 number of reviews = 7905\n",
      "movie_id = 2949 Dr. No (1962)  pearson correlation = 0.303972382012 number of reviews = 3132\n",
      "movie_id = 1097 E.T. the Extra-Terrestrial (1982)  pearson correlation = 0.296353445113 number of reviews = 13413\n",
      "movie_id = 2947 Goldfinger (1964)  pearson correlation = 0.295599616815 number of reviews = 4936\n",
      "\n",
      "\n",
      "Top 20 Movies You Won't Like if You Like Star Wars\n",
      "movie_id = 1483 Crash (1996)  pearson correlation = -0.0457768364225 number of reviews = 1055\n",
      "movie_id = 2427 Thin Red Line, The (1998)  pearson correlation = -0.0207112900357 number of reviews = 3121\n",
      "movie_id = 46 How to Make an American Quilt (1995)  pearson correlation = -0.00666903450593 number of reviews = 1068\n",
      "movie_id = 1884 Fear and Loathing in Las Vegas (1998)  pearson correlation = -0.00580796052152 number of reviews = 2423\n",
      "movie_id = 1464 Lost Highway (1997)  pearson correlation = -0.00457347012081 number of reviews = 1578\n",
      "movie_id = 2132 Who's Afraid of Virginia Woolf? (1966)  pearson correlation = -0.00242276641378 number of reviews = 1211\n",
      "movie_id = 1095 Glengarry Glen Ross (1992)  pearson correlation = 0.00133699883013 number of reviews = 2448\n",
      "movie_id = 2318 Happiness (1998)  pearson correlation = 0.00449961600169 number of reviews = 1526\n",
      "movie_id = 2926 Hairspray (1988)  pearson correlation = 0.00464842157315 number of reviews = 1317\n",
      "movie_id = 1964 Klute (1971)  pearson correlation = 0.00506554302623 number of reviews = 1044\n",
      "movie_id = 4848 Mulholland Drive (2001)  pearson correlation = 0.00630606484652 number of reviews = 2963\n",
      "movie_id = 2303 Nashville (1975)  pearson correlation = 0.00780074572255 number of reviews = 1027\n",
      "movie_id = 1295 Unbearable Lightness of Being, The (1988)  pearson correlation = 0.00874675796554 number of reviews = 1543\n",
      "movie_id = 2282 Pecker (1998)  pearson correlation = 0.0103263116452 number of reviews = 1004\n",
      "movie_id = 2505 8MM (1999)  pearson correlation = 0.0108473750064 number of reviews = 1539\n",
      "movie_id = 550 Threesome (1994)  pearson correlation = 0.0112322572098 number of reviews = 1233\n",
      "movie_id = 3535 American Psycho (2000)  pearson correlation = 0.011436967918 number of reviews = 2774\n",
      "movie_id = 1395 Tin Men (1987)  pearson correlation = 0.0127214289819 number of reviews = 1154\n",
      "movie_id = 662 Fear (1996)  pearson correlation = 0.0128012774349 number of reviews = 1143\n",
      "movie_id = 1094 Crying Game, The (1992)  pearson correlation = 0.0133143988399 number of reviews = 4967\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "def read_titles(movieDataPath):\n",
    "    \"\"\"\n",
    "        Read in the movie ids and their associated titles.\n",
    "        Returns a hashtable containing the association.\n",
    "        Note that the ids are ints.  \n",
    "    \"\"\"\n",
    "    titles = {}\n",
    "    for line in open(movieDataPath, 'r'):\n",
    "        tokens = line.split('::')\n",
    "        if len(tokens) >= 2:\n",
    "            (id, title) = tokens[:2]\n",
    "            titles[int(id)] = title\n",
    "    return titles\n",
    "\n",
    "\n",
    "\n",
    "def read_users(movieRatingspath):\n",
    "    \"\"\"\n",
    "        Generator function used to iterate over large file of \n",
    "        movie ratings.  File is assumed sorted by user\n",
    "        Generator will return dictionary of all user ratings\n",
    "    \"\"\"\n",
    "\n",
    "    user_ratings = {}     # dictionary from movie id to user rating\n",
    "    prev_user_id = -1     #  user id\n",
    "    \n",
    "    for line in open(movieRatingspath):\n",
    "        tokens = line.split('::')    # tokens is a list of the four values in each record\n",
    "        user_id = int(tokens[0])\n",
    "        movie_id = int(tokens[1])\n",
    "        rating = float(tokens[2])\n",
    "\n",
    "            \n",
    "        # if the current line represents a new user, process the previous user's ratings\n",
    "        if user_id != prev_user_id and len(user_ratings) > 0:\n",
    "            # this tells Python to use the value of user_ratings \n",
    "            # for the next iteration of the for loop at the bottom of this cell\n",
    "            #print \"user_id = \" + str(user_id), \"prev_user_id\" + str(prev_user_id)\n",
    "            yield user_ratings\n",
    "            \n",
    "            # mark new user as current user, clear out old user's ratings\n",
    "            user_ratings.clear()\n",
    "            prev_user_id = user_id        \n",
    "        \n",
    "        # record user rating\n",
    "        user_ratings[movie_id] = rating\n",
    "        prev_user_id = user_id\n",
    "    \n",
    "    # process the very last user in the file\n",
    "    #print \"user_id = \" + str(user_id), \"prev_user_id\" + str(prev_user_id)\n",
    "    yield user_ratings\n",
    "\n",
    "path = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/ratings.dat'  # replace this with the full path to the ratings file on your computer\n",
    "movieTitlePath = 'C:/Users/PAUL/Downloads/ml-10m/ml-10M100K/movies.dat'  # replace this with the full path to the ratings file on your computer\n",
    "\n",
    "\n",
    "every_X=collections.defaultdict(list)  # every_X will be a dictionary indexed by movie id.  Each dictionary item will be a list.  Each Item in the list\n",
    "           # will be a users movie rating for star wars\n",
    "every_Y=collections.defaultdict(list)  # every_Y will be a dictionary indexed by movie id.  Each dictionary item will be a list.  Each Item in the list\n",
    "           # will be a users movie rating for the indexed movie.\n",
    "\n",
    "every_correlation = {} # dictionary to hold movie correlations.  Indexed by movie id.  Each entry is the pearson correlation coefficient as a float.\n",
    "\n",
    "num_users = 0\n",
    "for user in read_users(path):\n",
    "    num_users += 1\n",
    "\n",
    "    #print user\n",
    "    \n",
    "    #determine if this user rated star wars.  movie id for start wars is 260\n",
    "    if 260 in user.keys():\n",
    "        #print 'Found target'\n",
    "        # get Star Wars Rating\n",
    "        starWarsRating=user[260]\n",
    "        for movieID in user.keys():\n",
    "            every_X[movieID].append(starWarsRating)\n",
    "            every_Y[movieID].append(user[movieID])\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\"\"\"        \n",
    "print('num_users:', num_users)\n",
    "print every_X\n",
    "print every_Y\n",
    "\"\"\"\n",
    "for movieID in every_X.keys():\n",
    "    #print movieID\n",
    "    if len(every_X[movieID]) > 1000:\n",
    "        #print \"movieID = \" +str(movieID), \" is length \" + str(len(every_X[movieID]))\n",
    "        #print  every_X[movieID]\n",
    "        #print  every_Y[movieID]\n",
    "        r_s, p = pearsonr(every_X[movieID], every_Y[movieID])\n",
    "        #print r_s, p\n",
    "        every_correlation[movieID] = r_s\n",
    "print len(every_correlation)\n",
    "\n",
    "\n",
    "movieTitles = read_titles(movieTitlePath)\n",
    "\n",
    "print \"Top 20 Movies You Might Like if You Like Star Wars\"\n",
    "for w in (sorted(every_correlation, key=every_correlation.get, reverse=True))[0:20]:\n",
    "  print \"movie_id = \" + str(w) + \" \" + movieTitles[w], \" pearson correlation = \" + str(every_correlation[w]) + \" number of reviews = \" + str(len(every_X[w]))\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Top 20 Movies You Won't Like if You Like Star Wars\"\n",
    "for w in (sorted(every_correlation, key=every_correlation.get, reverse=False))[0:20]:\n",
    "  print \"movie_id = \" + str(w) + \" \" + movieTitles[w], \" pearson correlation = \" + str(every_correlation[w]) + \" number of reviews = \" + str(len(every_X[w]))\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Analyzing time complexity\n",
    "\n",
    "**Introduction:** For this question, you will practice analyzing the time complexity of Python code. Although understanding time complexity well requires a dedicated course, when you finish this question you'll improve your intution about time complexity.\n",
    "\n",
    "**Completion time:** This question should require about 2 hours.\n",
    "\n",
    "**Point value:** 10 points.\n",
    "\n",
    "**Part A:** I've given you three Python functions below: f, g, and h. Each has a different time complexity. If you run the cell below in inotebook, it will report how long it takes to run f, g, and h. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1000\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 63 ms\n",
      "\n",
      "\n",
      "n = 2000\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 265 ms\n",
      "\n",
      "\n",
      "n = 4000\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 1.09 s\n",
      "\n",
      "\n",
      "n = 8000\n",
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "Wall time: 4.08 s\n",
      "\n",
      "\n",
      "n = 16000\n",
      "Wall time: 0 ns\n",
      "Wall time: 2 ms\n",
      "Wall time: 15.3 s\n",
      "\n",
      "\n",
      "n = 32000\n",
      "Wall time: 0 ns\n",
      "Wall time: 2 ms\n",
      "Wall time: 1min 2s\n",
      "n = 64000\n",
      "Wall time: 0 ns\n",
      "Wall time: 5 ms\n",
      "Wall time: 6min 16s\n",
      "n = 128000\n",
      "Wall time: 0 ns\n",
      "Wall time: 10 ms\n",
      "Wall time: 32min 18s\n"
     ]
    }
   ],
   "source": [
    "def f(n):\n",
    "    n += 1\n",
    "    return\n",
    "\n",
    "def g(n):\n",
    "    j = 1\n",
    "    for i in range(n):\n",
    "        j += 1\n",
    "    return\n",
    "\n",
    "def h(n):\n",
    "    j = 1\n",
    "    for i in range(n):\n",
    "        for k in range(n):\n",
    "            j += 1\n",
    "    return\n",
    "\n",
    "n = 1000\n",
    "print \"n = 1000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 2000\n",
    "print \"n = 2000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 4000\n",
    "print \"n = 4000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 8000\n",
    "print \"n = 8000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 16000\n",
    "print \"n = 16000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 32000\n",
    "print \"n = 32000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "\n",
    "n = 64000\n",
    "print \"n = 64000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)\n",
    "\n",
    "\n",
    "n = 128000\n",
    "print \"n = 128000\"\n",
    "%time f(n)\n",
    "%time g(n)\n",
    "%time h(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes f, g, and h to run as you increase n by factors of 2 (n=1000, 2000, 4000, etc.). Then do the following:\n",
    "\n",
    " 1. Record  (via markdown or comments) how long it takes to run f, g, and h for each value of n.\n",
    " 1. Describe the patterns do you see for each.\n",
    " 1. What time complexities do your observations suggest for f, g, and h?\n",
    " 1. Why does the code in each function generate the time complexities you observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1 Record (via markdown or comments) how long it takes to run f, g, and h for each value of n. </h4>\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>n</th><th>run time f</th><th>run time g</th><th>run time h</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1,000</td> <td>0</td>  <td>0</td>  <td>63</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>2,000</td> <td>0</td>  <td>0</td>  <td>256</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>4,000</td> <td>0</td>  <td>0</td>  <td>1,090</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>8,000</td> <td>0</td>  <td>0</td>  <td>4,080</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>16,000</td> <td>0</td>  <td>2</td>  <td>15,600</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>32,000</td> <td>0</td>  <td>2</td>  <td>62,000</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>64,000</td> <td>0</td>  <td>5</td>  <td>37,800</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>64,000</td> <td>0</td>  <td>10</td>  <td> 1,921,800.00 \n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Describe the patterns do you see for each.</h4>\n",
    "<p><b>- run time f</b> The run time of function f is so quick it is not measurable and doesn't increase with size of (n) </p>\n",
    "<p><b>- run time g</b> The run time of function g increases linearly with size of (n) </p>\n",
    "<p><b>- run time h</b> The run time of function h increases faster than linearly with size of (n) </p>\n",
    "\n",
    "<h4>3. What time complexities do your observations suggest for f, g, and h?</h4>\n",
    "<p><b>- time complexity f = </b>  O(1) </p>\n",
    "<p><b>- time complexity g = </b> O(n)</p>\n",
    "<p><b>- time complexity h = </b> O(n^2)</p>\n",
    "\n",
    "<h4>Why does the code in each function generate the time complexities you observed?</h4>\n",
    "<p>Function f does not iterate over the data so its run time will not depend on the size of n</p>\n",
    "<p>Function g iterates just once over the data so its run time will be linearly proporational to the size of n </p>\n",
    "<p>Function h has nested for loops so it will run proportionally to the square of the size of </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B:** Repeat the previous procedure from Part 1 for the functions p and q below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 10,000\n",
      "Wall time: 58 ms\n",
      "Wall time: 2 ms\n",
      "\n",
      "\n",
      "n = 20,000\n",
      "Wall time: 221 ms\n",
      "Wall time: 3 ms\n",
      "\n",
      "\n",
      "n = 40,000\n",
      "Wall time: 845 ms\n",
      "Wall time: 6 ms\n",
      "\n",
      "\n",
      "\n",
      "n = 80,000\n",
      "Wall time: 3.4 s\n",
      "Wall time: 13 ms\n",
      "\n",
      "\n",
      "n = 160,000\n",
      "Wall time: 13.6 s\n",
      "Wall time: 26 ms\n",
      "\n",
      "\n",
      "n = 320,000\n",
      "Wall time: 54.8 s\n",
      "Wall time: 56 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def p(n):\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        l.insert(0, 'foo')     # insert 'foo' to the beginning (the 0'th position)\n",
    "def q(n):\n",
    "    l = []\n",
    "    for i in range(n):\n",
    "        l.append('foo')        # append 'foo' to the end\n",
    "\n",
    "\n",
    "n = 10000\n",
    "print \"n = 10,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\"\n",
    "        \n",
    "n = 20000\n",
    "print \"n = 20,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\"       \n",
    "        \n",
    "n = 40000\n",
    "print \"n = 40,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\"\n",
    "\n",
    "print \n",
    "n = 80000\n",
    "print \"n = 80,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 160000\n",
    "print \"n = 160,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\"\n",
    "\n",
    "n = 320000\n",
    "print \"n = 320,000\"\n",
    "%time p(n)\n",
    "%time q(n)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time how long it takes p and q to run as you increase n by factors of 2 (this time start with n=10000). Then do the following:\n",
    " \n",
    " <h4>1. Record  (via markdown or comments) how long it takes to run p and q for each value of n.</h4>\n",
    " \n",
    "<table>\n",
    "<tr>\n",
    "<th>n</th><th>run time p</th><th>run time q</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>10,000</td> <td>58</td>  <td>2</td> \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>20,000</td> <td>221</td>  <td>3</td>  \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>40,000</td> <td>845</td>  <td>6</td> \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>80,000</td> <td>3,400</td>  <td>13</td> \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>160,000</td> <td>13,600</td>  <td>26</td> \n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>320,000</td> <td>54,800</td>  <td>56</td> \n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>\n",
    " \n",
    "<h4> 2. Describe the patterns do you see for each.</h4>\n",
    "<p>Function p is increasing close to proportionately to n-sqaured.  I put this data into excel and used the regression function with a power function specification and it almost perfectly fit this equation. y = 7E-07x^1.9788 R² = 0.9999 <p>\n",
    "<p>Function q is increasing at a linear rate.  Again, using Excel's regression function and specifying a linear model gives these results.  y = 0.0002x - 0.7065R² = 0.9985</p>\n",
    "\n",
    "\n",
    "<h4> 3. What time complexities do your observations suggest for p and q? </h4>\n",
    "<p>Function p has a time complexity of O(n^2>.  Function q has a time complexity of O(n)</p>\n",
    "\n",
    "\n",
    "<h4> 4. Why does the code in each function generate the time complexities you observed? Hint: Python's list uses an array that stores elements in consecutive memory locations. </h4>\n",
    "\n",
    "<p>Function q has a linear complexity because it simply needs to to traverse from the head of the list to the end and then append a new element to the end of the list.  Function p has a quadratic complexity because it must create an entire new data list data structure since the elements must fit in contiquous memory elements</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Estimate the time complexity of the code you wrote for question 2 in terms of the number of movie ratings n. You do not need to include an analysis of step 7 - only steps 1 through 6. Explain your answer.\n",
    "\n",
    "To help you, consider what actions you perform for each rating. What data structures are used? How fast are the operations you use on those data structures?\n",
    " \n",
    " 1. Your program should create an accumulator variable called `sums` that keeps track of the sum of ratings for each movies.  <b>=complexity O(1)</b>\n",
    " 1. Your program should create an accumulator variable called `counts` that keeps track of the count of ratings for each movie.<b>=complexity O(1)</b>\n",
    " 1. Your program should stream through each movie rating.<b>=complexity O(n)</b>\n",
    " 1. For each rating, your program should update the two accumulator data structures.=complexity O(n)</b>\n",
    " 1. Your program should create a dictionary called `robust_means` whose keys will be movie ids and values will be robust means.<b>=complexity O(1)</b>\n",
    " 1. Your program should fill the `robust_means` dictionary using data in the `sums` and `counts` accumulators.<b>=complexity O(n)</b>\n",
    " \n",
    "<p><b>Overall the first 6 steps of this program would have a linear time complexity proportional to the number of movie ratings n, this would be O(n).</b></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Estimate the time complexity of the code you wrote for question 3 in terms of the number of movie ratings n. You do not need to include an analysis of step 6 - only steps 1 through 5. \n",
    "\n",
    "To make your analysis simpler, presume *every* user has rated Star Wars. How would you adjust your analysis to account for the fact that only some people rate Star Wars?\n",
    "\n",
    "Make sure to explain your answer.\n",
    "\n",
    "As a hint, consider what actions you perform for each rating. What data structures are used? How fast are the operations you use on those data structures?\n",
    "\n",
    " 1. Create two data structures to hold rating lists for every movie: `every_X` and `every_Y`. <b>=complexity O(1)</b>\n",
    "    Carefully plan the contents of these data structures and describe them in your source code.\n",
    " 1. While reading in users via the `read_users()` method, skip all users who have not rated Star Wars (movie id 260).<b>=complexity O(n) </b>\n",
    " 1. For each user who has rated Star Wars, update `every_X` and `every_Y` for each movie they have rated.<b>=complexity O(n)</b>\n",
    " 1. After you have finished processing all users, create a dictionary called `every_correlation`.<b>=complexity O(1)</b>\n",
    " 1. Fill `every_correlation` with the correlations for every movie with at least 1000 ratings using the data in `every_X` and `every_Y`.<b>=complexity O(n)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Overall the first 5 steps of this program would have a quadratic time complexity proportional to the number of users u times  movie ratings n.  Because step 3 is nested in side step 2,  this would be O(n^2). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
