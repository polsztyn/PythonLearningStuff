{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to text-mining in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson will introduce you to natural language processing (NLP) in Python. Your IPython installation comes bundled with NLP power tools (NLTK and Scikit-Learn). Instead of jumping right into these tools, we'll first explore the basic concepts using native Python before moving onto the tools.\n",
    "\n",
    "**Objectives**:\n",
    "\n",
    "* Understand basic NLP data representations, such as documents and term frequency vectors, and sparse vectors.\n",
    "* Learn how to build term frequency matrixes from documents in Python.\n",
    "* Understand why L-2 vector normalization and Tf-Idf transformation are important, and learn to implement them in Python.\n",
    "* Learn how to use Sci-kit learn to accomplish basic NLP tasks robustly and efficiently.\n",
    "* Learn how to calculate the similarity between two vectors using cosine similarity.\n",
    "\n",
    "**Acknowledgement:** \n",
    "\n",
    "* Portions of this lesson are based on a tutorial by [Rebecca Weiss](http://www.stanford.edu/~rjweiss/), with her permission. Thanks, Rebecca!\n",
    "* The example documents are based on http://stackoverflow.com/a/1750187."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Basic data representation\n",
    "\n",
    "* **Objectives**: Understand basic NLP data representations in Python, such as documents, term frequency vectors, and sparse vectors. Learn how to build term frequency matrixes from documents in Python.\n",
    "* **Time estimate**: 30 minutes, including time for Practice Exercise 1.\n",
    "* **Further reading:** http://en.wikipedia.org/wiki/Bag-of-words_model\n",
    "\n",
    "Imagine you want to analyze a list of **documents** that are strings. Depending on your application, a document might represent a single Wikipedia article, Tweet, or Facebook status post. You could naturally store this in Python as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = [\n",
    "    'Julie loves me more than Linda loves me',\n",
    "    'Jane likes me more than Julie loves me',\n",
    "    'He likes basketball more than baseball'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In text analysis you represent these documents as a matrix, where each **row** is  document, and the columns are associated with words. Since we want to compare documents with each other, we need to share a common set of columns. Thus, the matrix includes a **column** for every word that appears in at least one document. The **value** in a particular cell is the number of times the word appears in that document:\n",
    "\n",
    "    baseball    basketball    he    Jane    Julie    Linda    likes    loves    me    more    than\n",
    "       0            0          0      0       1        1        0        2       2      1       1\n",
    "       0            0          0      1       0        1        0        2       2      1       1   \n",
    "       1            1          1      0       0        0        1        0       0      1       1\n",
    "\n",
    "This matrix is typically called the **term frequency** (i.e. word count) matrix. In Python you might naturally represent a single document's vector as a list of ints, and the entire matrix as a list of lists.\n",
    "\n",
    "Notice how many zeros appear in the term frequency matrix? One-third of the cells are zero. How would a real matrix look? As a thought exercise, consider the term frequency matrix for Wikipedia articles? In English, there are about 4 million articles, so there would be four million rows, and there would probably be more than hundred millions columns (distinct terms). Any one articles only uses a small fraction of terms, so over 99% of the cells would contain zeros.\n",
    "\n",
    "To efficiently store these **sparse matrices** with many zeros, we represent each document vector as a dict of ints, where the keys are terms and the values are frequencies. This little bit of code would build up the sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}),\n",
      " defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1})]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def make_tf_vector(doc):\n",
    "    \"\"\"\n",
    "        Calculates the term frequency of a document.\n",
    "        Given a string, splits it into words on whitespace.\n",
    "        Returns a dictionary mapping words to their frequency.\n",
    "    \"\"\"\n",
    "    v = defaultdict(int)\n",
    "    for term in doc.split():\n",
    "        v[term] += 1\n",
    "    return v\n",
    "\n",
    "tf_matrix = []\n",
    "for doc in docs:    \n",
    "    v = make_tf_vector(doc)    \n",
    "    tf_matrix.append(v)\n",
    "\n",
    "pprint(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hint:*** *the pprint.pprint function \"prettily\" prints values with human friendly indenting and line breaks. It's useful when displaying complex nested data structures.*\n",
    "\n",
    "**Practice Exercise 1:**\n",
    "What does `doc.split()` precisely do in the `make_tf_vector` function above? What are its limitations in this setting? How might you improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Julie', 'loves', 'me', 'more', 'than', 'Linda', 'loves', 'me']\n",
      "['Bob', 'is', 'a', 'good', 'guy.', 'As', 'a', 'guy', \"he's\", 'good']\n",
      "['Bob', 'is', 'a', 'Good', 'Guy.', 'As', 'a', 'guy', \"he's\", 'good!']\n"
     ]
    }
   ],
   "source": [
    "# Practice Excercise 1 : What does doc.split() precisely do in the make_tf_vector function above?\n",
    "#doc.split breaks apart a string and returns a list of the words.  by default split divides on all whitespace characters.\n",
    "a = docs[0].split()\n",
    "pprint (a)\n",
    "\n",
    "#What are its limitations in this setting? How might you improve it?\n",
    "# one limitation is periods and other punction marks will be parsed together into a phrase.\n",
    "sentence = \"Bob is a good guy.  As a guy he's good\"\n",
    "a = sentence.split()\n",
    "pprint (a)\n",
    "\n",
    "# This word splitter also would falsely distinguish between two words in differenct cases.\n",
    "sentence = \"Bob is a Good Guy.  As a guy he's good!\"\n",
    "a = sentence.split()\n",
    "pprint (a)\n",
    "\n",
    "# regular expressions could be used to convert raw input text and remove all non alpha characters\n",
    "# and change the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Vector normalization\n",
    "\n",
    "* ** Objective:** Understand why L-2 vector normalization is important and learn to implement it in Python.\n",
    "* ** Time estimate:** 30 minutes, including Immediate Feedback Question 2, but not Assignment Question 1.\n",
    "* **Further reading:**\n",
    "  * http://www.dummies.com/how-to/content/finding-the-unit-vector-of-a-vector.html\n",
    "  * http://mathworld.wolfram.com/L2-Norm.html\n",
    "\n",
    "Lets presume we have two documents:\n",
    "\n",
    "  * Document A has 50 words, and \"dog\" appears 4 times.\n",
    "  * Document B has 100 words and \"dog\" appears 8 times. \n",
    "\n",
    "Is \"dog\" more important to document B than A? According to term frequency, yes (tf=8 for B versus tf=2 for A). However, your intuition probably tells you that \"dog\" makes up 8% of words for both documents, so it is similarly important to both.\n",
    "\n",
    "**Vector normalization** ensures that term frequency values are scaled relative to the size of the document. Although people use many different techniques for normalizing vectors, the most popular translates a vector into a [unit vectors](http://www.mathsisfun.com/algebra/vector-unit.html) by making sure its euclidean length is 1.0. \n",
    "\n",
    "The euclidean length, or **L-2 norm** of a vector, is the square root of the square of the values in the vector. For example, the l2norm of the first document is:\n",
    "\n",
    "$$l2norm(\\mbox{docs[0]}) = \\sqrt{2*2 + 1*1 + 2*2 + 1*1 + 1*1 + 1*1} = \\sqrt{12} = 3.464$$\n",
    "\n",
    "**Practice Exercise 2:** The Python code for the l2norm function appears below. This version uses a [list comprehension](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html) (an advanced Python technique). Rewrite the l2norm function using a standard \"for-each\" loop. Your norm results should match the output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}), 'is', 3.4641016151377544)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'me': 2, 'Julie': 1, 'likes': 1, 'loves': 1, 'Jane': 1, 'than': 1, 'more': 1}), 'is', 3.162277660168379)\n",
      "('l2norm of', defaultdict(<type 'int'>, {'basketball': 1, 'baseball': 1, 'likes': 1, 'He': 1, 'than': 1, 'more': 1}), 'is', 2.449489742783178)\n"
     ]
    }
   ],
   "source": [
    "def l2norm(tf_vector):\n",
    "    \"\"\"\n",
    "        Returns the l2-norm (i.e. Euclidean length) of a vector.\n",
    "    \"\"\"\n",
    "    return sum([x*x for x in tf_vector.values()]) ** 0.5\n",
    "\n",
    "for tf_vector in tf_matrix:\n",
    "    norm = l2norm(tf_vector)\n",
    "    print('l2norm of' , tf_vector, 'is', norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have calculated the L2 length of each vector, we divide all the values by the length. Again, the code below uses a list comprehension to build up the normalized vector. For **Assignment Question 1** you will rewrite `normalize` below and replace the list comprehension with a standard for-each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
      "3.46410161514\n",
      "{'me': 0.5773502691896258, 'Julie': 0.2886751345948129, 'loves': 0.5773502691896258, 'Linda': 0.2886751345948129, 'than': 0.2886751345948129, 'more': 0.2886751345948129}\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def normalize(vector):\n",
    "    \"\"\"\n",
    "        Given a dictionary representing a sparse vector, returns a new rescaled vector.\n",
    "        The new vector contains values from the original vector rescaled by a constant.\n",
    "        The new vector will have an l2-norm of 1.0.\n",
    "    \"\"\"\n",
    "    norm = l2norm(vector)\n",
    "    if norm == 0.0:\n",
    "        return dict(vector)\n",
    "    \n",
    "    return dict([(term, tf/norm) for (term, tf) in vector.items()])\n",
    "\n",
    "v = {'me': 2, 'Julie': 1, 'loves': 2, 'Linda': 1, 'than': 1, 'more': 1}\n",
    "print(v)\n",
    "print(l2norm(v))\n",
    "print(normalize(v))\n",
    "print(l2norm(normalize(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. TF-IDF weighting\n",
    "\n",
    "* **Objective:** Understand tf-idf weighting, why it is important and learn to implement it in Python.\n",
    "* **Time estimate:** 25 minutes, not including Assignment Questions 2 and 3.\n",
    "* **Further reading:**\n",
    "    * http://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/\n",
    "    * http://www.tfidf.com/\n",
    "\n",
    "Words aren't all equally informative. \n",
    "Imagine you calculated the term frequency vector for the Wikipedia article for [Python](http://en.wikipedia.org/wiki/Python_(programming_language).\n",
    "What would you expect the largest values to be?\n",
    "I'll eliminate the suspense... they appear below.\n",
    "\n",
    "    word       frequency\n",
    "    python        345\n",
    "    the           268\n",
    "    and           201\n",
    "    retrieved     96\n",
    "    for           93\n",
    "\n",
    "Does your intuition tell you that \"the\" should be the second largest value in the term frequency vector?\n",
    "Probably not, because it is an extremely common word!\n",
    "\n",
    "**Tf-Idf weighting** (Term frequency - Inverse document frequency) controls for overall word popularity by weighting words that occur in many documents lower. The formula for tf-idf appears below, and relies on three values. The first, **term frequency (tf)**, you already know. The second value, **document frequency (df)**, is the number of documents that contain that word (e.g. number of Wikipedia articles that contain 'python'. The last, **total documents (td)** is simple the number of documents in the corpus (e.g. the total number of Wikipedia articles).\n",
    "\n",
    "The final formula for tf-idf follow. Note that we typically apply l2- normalization **after** calculating the tf-idf vector.\n",
    "\n",
    "$$ TfIdf(doc, word) = tf(doc, word) * log\\frac{td}{1 + df(word)}$$\n",
    "\n",
    "Let's write Python code to calculate tf-idf. We'll start by calculating the document frequency, which is a dictionary mapping terms to the number of documents in which they appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'me': 2, 'basketball': 1, 'Julie': 2, 'baseball': 1, 'likes': 2, 'loves': 2, 'Jane': 1, 'Linda': 1, 'He': 1, 'than': 3, 'more': 3})\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from collections import defaultdict\n",
    "\n",
    "def make_doc_frequency(docs):\n",
    "    \"\"\"\n",
    "        Given a collection of documents (Strings), \n",
    "        returns a dictionary mapping each word to the number of times it appears.        \n",
    "        Words are split on whitespace.\n",
    "    \"\"\"\n",
    "    df = defaultdict(int)\n",
    "    for d in docs:\n",
    "        for term in set(d.split()):\n",
    "            df[term] += 1\n",
    "    return df\n",
    "\n",
    "print(make_doc_frequency(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Question 2:** Asks you about the importance of `set()` in the code above, and asks you about the time complexity of the function.\n",
    "\n",
    "**Assignment Question 3** Asks you to implement a `make_tf_idf_vector` function that returns the tf-idf and normalized feature vector for a document. The correct tf-idf scores appear below:\n",
    "\n",
    "        Julie loves me more than Linda loves me\n",
    "                  Linda: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=2 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        Jane likes me more than Julie loves me\n",
    "                   Jane: tf=1 tf-idf=+0.706\n",
    "                     me: tf=2 tf-idf=+0.000\n",
    "                  Julie: tf=1 tf-idf=+0.000\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                  loves: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.501\n",
    "                   more: tf=1 tf-idf=-0.501\n",
    "        \n",
    "        He likes basketball more than baseball\n",
    "             basketball: tf=1 tf-idf=+0.500\n",
    "               baseball: tf=1 tf-idf=+0.500\n",
    "                     He: tf=1 tf-idf=+0.500\n",
    "                  likes: tf=1 tf-idf=+0.000\n",
    "                   than: tf=1 tf-idf=-0.354\n",
    "                   more: tf=1 tf-idf=-0.354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4. Writing robust sparse vector code using scikit-learn\n",
    "\n",
    "\n",
    "* **Objective:** Learn how to use sci-kit learn to implement robust NLP vector algorithms.\n",
    "* **Time estimate:** 40 minutes, not including Assignment Questions 4.\n",
    "* **Further reading:**\n",
    "    * http://css.dzone.com/articles/machine-learning-text-feature\n",
    "    * http://css.dzone.com/articles/machine-learning-text-feature-0\n",
    "    * http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\n",
    "You now understand how to calculate tf-idf vectors for a document by hand. However, if you try to apply this to a real dataset by hand, you'll notice lots of anomalies. For example, you may want to make your counting case insensitive. Or perhaps you want to *tokenize* documents from strings to words more accurately by capturing punctuation, etc. Luckily, natural language programming experts have already developed carefully tuned tf-idf implementations in sci-kit learn. \n",
    "\n",
    "###4.1. Building up the lexicon with sci-kit learn\n",
    "\n",
    "In sci-kit learn (and many other machine learning libraries), a document's sparse vector is called its **feature vector**. The first step to in creating tf-idf feature vetors in sci-kit learn is to build up a **lexicon**, or a list of all the distinct terms across all documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Lexicon:', [u'baseball', u'basketball', u'he', u'jane', u'julie', u'likes', u'linda', u'loves', u'me', u'more', u'than'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(docs)\n",
    "lexicon =  count_vectorizer.get_feature_names()\n",
    "print(\"Lexicon:\", lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.2. Creating sparse vectors with sci-kit learn\n",
    "\n",
    "Recall that in Part 1 we represented term frequency vectors as dicts, with terms as keys and frequencies as values. Sci-kit learn makes the sparse vectors more space-efficient by replacing the terms with contiguous ids assigned to each term in the lexicon above ('baseball' = 0, 'basketball' = 1, ...). Thus, the tf matrix, represented densely, becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 2 2 1 1]\n",
      " [0 0 0 1 1 1 0 1 2 1 1]\n",
      " [1 1 1 0 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "sk_tf_matrix = count_vectorizer.transform(docs)\n",
    "print(sk_tf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert back to our representation from part 1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document\n",
      "\tjulie (id=4) : 1.000\n",
      "\tlinda (id=6) : 1.000\n",
      "\tloves (id=7) : 2.000\n",
      "\tme (id=8) : 2.000\n",
      "\tmore (id=9) : 1.000\n",
      "\tthan (id=10) : 1.000\n",
      "document\n",
      "\tjane (id=3) : 1.000\n",
      "\tjulie (id=4) : 1.000\n",
      "\tlikes (id=5) : 1.000\n",
      "\tloves (id=7) : 1.000\n",
      "\tme (id=8) : 2.000\n",
      "\tmore (id=9) : 1.000\n",
      "\tthan (id=10) : 1.000\n",
      "document\n",
      "\tbaseball (id=0) : 1.000\n",
      "\tbasketball (id=1) : 1.000\n",
      "\the (id=2) : 1.000\n",
      "\tlikes (id=5) : 1.000\n",
      "\tmore (id=9) : 1.000\n",
      "\tthan (id=10) : 1.000\n"
     ]
    }
   ],
   "source": [
    "for row in sk_tf_matrix:\n",
    "    print('document')\n",
    "    for i in range(len(row.data)):\n",
    "        index = row.indices[i]        \n",
    "        term = lexicon[index]\n",
    "        val = row.data[i]\n",
    "        print('\\t%s (id=%d) : %.3f' % (term, index, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Question 4:** Asks you to write a function that converts a sci-kit learn sparse vector into a native python sparse vector representation (a dict whose keys are terms and values are frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Creating Tf-Idf vectors with sci-kit learn\n",
    "\n",
    "Note that sci-kit learn has a TfidfTransformer object that makes your life substantially easier. The `fit()` method of the transformer object calculates and stores the document frequencies so that it can compute the tf-idf tranformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Train a tf-idf transformer on the dataset\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(sk_tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform function then converts a matrix (or single tf vector) into its tf-idf representation. Ski-kit learn's tf-idf implementation [differs from than the standard idf calculation](http://stackoverflow.com/questions/18687879/error-in-computing-text-similarity-using-scikit-learn/18692538#18692538), so the values in the matrix are a bit different from ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.28945906  0.\n",
      "   0.38060387  0.57891811  0.57891811  0.22479078  0.22479078]\n",
      " [ 0.          0.          0.          0.41715759  0.3172591   0.3172591\n",
      "   0.          0.3172591   0.6345182   0.24637999  0.24637999]\n",
      " [ 0.48359121  0.48359121  0.48359121  0.          0.          0.36778358\n",
      "   0.          0.          0.          0.28561676  0.28561676]]\n"
     ]
    }
   ],
   "source": [
    "sk_tf_idf_matrix = tfidf.transform(sk_tf_matrix)\n",
    "print sk_tf_idf_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4.4. Sci-kit learn and arithmetic\n",
    "\n",
    "A sci-kit \"sparse row\" is actually a 1 x n sparse matrix, where n is the number of terms in the lexicon. This row can be conveniently manipulated using arithmetic operations such as `+`, `-`, and `*`.  \n",
    "\n",
    "For example, imagine we wanted to reconstruct the **overall term frequency vector** across all documents in the dataset. We would first create an empty vector (remember that a vector is a 1 x n matrix). To do so, we would call the numpy.zeros function, which takes a 2-tuple representing the size (rows, columns) of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# creates a vector of zeros for each term\n",
    "overall_tf_vector = numpy.zeros((1, len(lexicon)))\n",
    "print(overall_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we could simply add each document's sparse matrix row to the overall tf vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'baseball', u'basketball', u'he', u'jane', u'julie', u'likes', u'linda', u'loves', u'me', u'more', u'than']\n",
      "[[ 1.  1.  1.  1.  2.  2.  1.  3.  4.  3.  3.]]\n"
     ]
    }
   ],
   "source": [
    "for row in sk_tf_matrix:\n",
    "    overall_tf_vector = overall_tf_vector + row\n",
    "print(lexicon)\n",
    "print(overall_tf_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Measuring vector similarity\n",
    "\n",
    "* **Objective:** Learn how to calculate the similarity of sparse feature vectors.\n",
    "* **Time estimate:** 20 minutes\n",
    "* **Further reading:**\n",
    "    * http://pyevolve.sourceforge.net/wordpress/?p=2497\n",
    "\n",
    "We have three documents in our example. Lets say we wanted to know which two were most similar. If we have unit vectors (we do!), a common method is to compute the [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity) of those vectors.  The formula for cosine similarity, which ranges from -1.0 (opposites) to 1.0 (the same), follows.\n",
    "\n",
    "$$cosineSimilarity(X,Y) = \\frac{X \\cdot Y}{|X| \\hspace{0.4em} |Y|}$$\n",
    "\n",
    "Where $|X|$ is the l2-norm of X and $X \\cdot Y$ is the dot product of X and Y. The **dot product** multiplies the values in each column for X and Y and sums them up.  Note that for a unit-vector $X$, $|X|$ = 1, so $cosineSimilarity(X,Y) = X \\cdot Y$.\n",
    "\n",
    "Returning to our original non-sci-kit tf-idf representation for clarity, we could implement the dot product as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_tf_idf_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-aac26d6a9e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msum_prods\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0md0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_tf_idf_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0md1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_tf_idf_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0md2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_tf_idf_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_tf_idf_vector' is not defined"
     ]
    }
   ],
   "source": [
    "def dot(v1, v2):\n",
    "    sum_prods = 0.0\n",
    "    for term in v1:\n",
    "        if term in v2:\n",
    "            sum_prods += v1[term] * v2[term]\n",
    "    return sum_prods\n",
    "\n",
    "d0 = make_tf_idf_vector(td, df, docs[0])\n",
    "d1 = make_tf_idf_vector(td, df, docs[1])\n",
    "d2 = make_tf_idf_vector(td, df, docs[2])\n",
    "\n",
    "print(docs)\n",
    "print(dot(d0, d1))\n",
    "print(dot(d0, d2))\n",
    "print(dot(d1, d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So \"Julie loves me more than Linda loves me\" and \"Jane likes me more than Julie loves me\" are more similar to each other (dot = 0.50) than \"He likes basketball more than baseball\".\n",
    "\n",
    "In sci-kit learn this can be accomplished using the `dot()` method of sparse matrices. Two \"gotchas\" are a) the second matrix to `dot` must be transposed and b) `dot()` returns a numpy array, so you must get the value contained within it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753602532225\n",
      "0.128408027002\n",
      "0.128408027002\n"
     ]
    }
   ],
   "source": [
    "def sk_dot(v1, v2):\n",
    "    return v1.dot(v2.transpose())[0,0]\n",
    "\n",
    "d0 = sk_tf_idf_matrix[0]\n",
    "d1 = sk_tf_idf_matrix[1]\n",
    "d2 = sk_tf_idf_matrix[2]\n",
    "print(sk_dot(d0, d1))\n",
    "print(sk_dot(d0, d2))\n",
    "print(sk_dot(d0, d2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because scipy's uses a [non-standard implementation of tf-idf](http://stackoverflow.com/questions/18687879/error-in-computing-text-similarity-using-scikit-learn/18692538#18692538), the values are slightly unusual. However, the ordering of the values is the same.   **Assignment Question 5** Asks you to rewrite the Python code above using loops so it is more generally useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. Additional Concepts\n",
    "\n",
    "* In Assignment Question 6 you will write code to identify the most distinctive terms for a particular document.\n",
    "* In Assignment Question 7 you will write code to find the most similar documents.\n",
    "* In Assignment Question 8 you will write code to cluster documents using K-means clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
